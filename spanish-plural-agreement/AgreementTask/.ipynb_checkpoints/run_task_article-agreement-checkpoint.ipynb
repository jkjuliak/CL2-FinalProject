{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d40805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "### Basic logistical, wrangling, & visualization imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### Model imports\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e8fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define useful functions\n",
    "\n",
    "def make_sentences_df(dct_noun,df_noun):\n",
    "    \"\"\"Creates sentence templates masking what should be a definite or indefinite, plural or singular article\n",
    "    INPUTS: dct_noun: dictionary, noun:article pairs, the article is lowercase, singular, and gendered--this \n",
    "                      was hand-annotated from native Caribbean Spanish speaker intuition and checks in online\n",
    "                      dictionary of the Real Academia Espa√±ola; if there are any concerns about gendered article \n",
    "                      errors, check here first\n",
    "            df_noun: pandas dataframe, cols: whole_word (plural), root (becomes our lemma), affix (s or es)--\n",
    "                     this df is drawn from a noun subset of AnCora Tree Bank\"\"\"\n",
    "    \n",
    "    gather_df = []\n",
    "    for noun in tqdm(dct_noun.keys()): \n",
    "        \n",
    "        subdf = df_noun[df_noun['whole_word'] == noun]\n",
    "\n",
    "        ### set up singular & plural template components\n",
    "        sing_art_def = dct_noun[noun]\n",
    "        sing_art_def = str(sing_art_def[0].upper() + sing_art_def[1])\n",
    "\n",
    "        sing_subj = subdf['root'].values[0]\n",
    "\n",
    "        plur_subj = noun\n",
    "\n",
    "        ### set up target definite and indefinite articles\n",
    "        if sing_art_def == 'El':\n",
    "\n",
    "            plur_art_def = 'Los'\n",
    "            plur_art_indef = 'Unos'\n",
    "\n",
    "            sing_art_indef = 'Un'\n",
    "\n",
    "\n",
    "        elif sing_art_def == 'La': \n",
    "\n",
    "            plur_art_def = 'Las'\n",
    "            plur_art_indef = 'Unas'\n",
    "\n",
    "            sing_art_indef = 'Una'\n",
    "\n",
    "\n",
    "        ### set up composite template components\n",
    "        affix = subdf['affix'].values[0]\n",
    "        comp_subj = sing_subj +'##'+ affix\n",
    "\n",
    "\n",
    "        ### create all sentence templates\n",
    "        article_types = ['definite','indefinite']\n",
    "        for article_type in article_types: \n",
    "\n",
    "            sing_sentence_template = '[MASK] ' + sing_subj \n",
    "\n",
    "            plur_sentence_template = '[MASK] ' + plur_subj \n",
    "\n",
    "            comp_sentence_template = '[MASK] ' + comp_subj \n",
    "\n",
    "\n",
    "            ### set up all the column/row entries you will store per lemma\n",
    "\n",
    "            all_wordforms = [sing_subj,\n",
    "                             plur_subj,\n",
    "                             comp_subj]\n",
    "\n",
    "            all_word_number = ['sing',\n",
    "                               'plur',\n",
    "                               'plur'] \n",
    "\n",
    "            all_n_tokens = [len(tokenizer.encode(sing_subj,add_special_tokens=False)),\n",
    "                            len(tokenizer.encode(plur_subj,add_special_tokens=False)),\n",
    "                            len(tokenizer.encode(sing_subj,add_special_tokens=False)) + len(tokenizer.encode(['##'+affix],add_special_tokens=False))\n",
    "                           ]\n",
    "\n",
    "            all_sentence_templates = [sing_sentence_template,\n",
    "                                      plur_sentence_template,\n",
    "                                      comp_sentence_template]\n",
    "            \n",
    "            #this is a super important column! distinguishes the default tokenization from \n",
    "            #our artificially imposed tokenization scheme\n",
    "            all_tokenization_types = ['default',\n",
    "                                      'default',\n",
    "                                      'artificial'\n",
    "                                     ]\n",
    "\n",
    "            if article_type == 'definite': \n",
    "                sing_art = sing_art_def\n",
    "                plur_art = plur_art_def\n",
    "\n",
    "            elif article_type == 'indefinite': \n",
    "                sing_art = sing_art_indef\n",
    "                plur_art = plur_art_indef\n",
    "\n",
    "            d = {'lemma': np.repeat(sing_subj,len(all_sentence_templates)),\n",
    "                 'word_form': all_wordforms,\n",
    "                 'word_number': all_word_number,\n",
    "                 'n_tokens': all_n_tokens,\n",
    "                 'tokenization_type': all_tokenization_types,\n",
    "                 'sentence': all_sentence_templates,\n",
    "                 'target_ART_sing': sing_art,\n",
    "                 'target_ART_plur': plur_art,\n",
    "                 'article_type': article_type,\n",
    "                 'affix': np.repeat(affix,len(all_sentence_templates))\n",
    "                }\n",
    "\n",
    "            gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "    sentence_df = pd.concat(gather_df,ignore_index=True)\n",
    "    return sentence_df\n",
    "\n",
    "\n",
    "### Model Predictions \n",
    "\n",
    "def find_sublist_index(list, sublist):\n",
    "    \"\"\"Find the first occurence of sublist in list.\n",
    "    Return the start and end indices of sublist in list.\n",
    "    Used to find index of [MASK] in template sentences.\n",
    "\n",
    "    h/t GPT-3-codex for writing this.\"\"\"\n",
    "\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == sublist[0] and list[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_article_predictions(df,data_source): \n",
    "    \"\"\"Predict the likelihood of target definite/indefinite and singular/plural articles\n",
    "       Will assume you've already loaded and defined your tokenizer and model.\n",
    "       Will iterate row by row in your dataframe `df`, containing cols for masked sentences,\n",
    "       the corresponding lemma and plural forms being tested. When it comes across a row\n",
    "       with column `tokenizer_type` label `artificial`, will shunt you to a process that creates\n",
    "       the inputs to the model by hand (if there are any issues/concerns with how the artificial\n",
    "       tokenization proceeds and leads to predictions, check the following `if` statement: \n",
    "       `if row['tokenization_type'] == 'artificial'`)\n",
    "       \n",
    "       INPUTS: df, pandas dataframe, cols for lemma, word_number (plural, singular), tokenization_type\n",
    "                   (artificial/default), masked sentence, target singular and plural articles to \n",
    "                   get probabilities for filling in the [MASK], and others\"\"\"\n",
    "\n",
    "    gather_df = []\n",
    "    gather_debug = []\n",
    "    for (_,row) in tqdm(df.iterrows(),total=df.shape[0]):\n",
    "\n",
    "        target_article_sing = row['target_ART_sing']\n",
    "        target_article_plur = row['target_ART_plur']\n",
    "\n",
    "\n",
    "        #tokens for each article type \n",
    "        token_article_sing = tokenizer.encode(target_article_sing,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        token_article_plur = tokenizer.encode(target_article_plur,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        #token for mask\n",
    "        token_mask = tokenizer.encode('[MASK]',\n",
    "                                      add_special_tokens=False\n",
    "                                     )\n",
    "\n",
    "\n",
    "        ### Set up your representation of the sentence for the \n",
    "        #.  model\n",
    "\n",
    "        if row['tokenization_type'] == 'artificial': \n",
    "\n",
    "            if '##es' in row['word_form']:\n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##es\"])\n",
    "\n",
    "\n",
    "            elif '##s' in row['word_form']: \n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##s\"])\n",
    "\n",
    "\n",
    "            #token for singular form\n",
    "            lemma = row['lemma']\n",
    "            token_lemma = tokenizer.encode(lemma,\n",
    "                                           add_special_tokens=False\n",
    "                                          )\n",
    " \n",
    "            #token for special start\n",
    "            start = '[CLS]'\n",
    "            token_start = tokenizer.encode(start,\n",
    "                                           add_special_tokens=False)\n",
    "            \n",
    "\n",
    "            #token for special end\n",
    "            ending = '[SEP]'\n",
    "            token_end = tokenizer.encode(ending,\n",
    "                                         add_special_tokens=False)\n",
    "            \n",
    "            \n",
    "            ### Collect your tokens into a list that you will then flatten\n",
    "            #   prior to converting to tensor\n",
    "            bulky_token_list = [token_start,\n",
    "                                token_mask,\n",
    "                                token_lemma,\n",
    "                                token_affix,\n",
    "                                token_end\n",
    "                                ]\n",
    "            flat_token_list = [item for sublist in bulky_token_list for item in sublist]\n",
    "            token_idx = torch.tensor([flat_token_list])\n",
    "            \n",
    "\n",
    "            inputs = {'input_ids': token_idx,\n",
    "                      'token_type_ids': torch.zeros_like(token_idx),\n",
    "                      'attention_mask': torch.ones_like(token_idx)\n",
    "                     }\n",
    "\n",
    "        elif row['tokenization_type']=='default': \n",
    "\n",
    "            inputs = tokenizer(row['sentence'],\n",
    "                               return_tensors='pt',\n",
    "                               add_special_tokens=True\n",
    "                              )\n",
    "            \n",
    "        model_token_inputs = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0])\n",
    "        model_token_inputs = ' , '.join(model_token_inputs)\n",
    "\n",
    "        ### Predict the item that should fill in the mask!\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        #find the index of the mask in sentence\n",
    "        midx = find_sublist_index(inputs[\"input_ids\"][0].tolist(),\n",
    "                                  token_mask)\n",
    "                                 \n",
    "        masked_token_logits = outputs.logits[0][midx[0]]\n",
    "        masked_token_probs = torch.softmax(masked_token_logits, dim=0)\n",
    "\n",
    "\n",
    "        prob_article_sing = masked_token_probs[token_article_sing].item()\n",
    "        prob_article_plur = masked_token_probs[token_article_plur].item()\n",
    "\n",
    "        prob_list = [prob_article_sing, prob_article_plur]\n",
    "        article_list = [target_article_sing, target_article_plur]\n",
    "    \n",
    "\n",
    "    \n",
    "        ### Store your results\n",
    "\n",
    "        d = {'lemma': np.repeat(row['lemma'],len(prob_list)),\n",
    "             'word_form': np.repeat(row['word_form'],len(prob_list)),\n",
    "             'word_number': np.repeat(row['word_number'],len(prob_list)),\n",
    "             'n_tokens': np.repeat(row['n_tokens'],len(prob_list)),\n",
    "             'tokenization_type': np.repeat(row['tokenization_type'],len(prob_list)),\n",
    "             'article_probs': prob_list,\n",
    "             'article_number': ['singular','plural'],\n",
    "             'article_type': np.repeat(row['article_type'],len(prob_list)),\n",
    "             'article': article_list,\n",
    "             'affix': row['affix'],\n",
    "             'sentence': row['sentence'],\n",
    "             'model_token_inputs': model_token_inputs,\n",
    "             'source': data_source\n",
    "            }\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "\n",
    "        debug_d = {'lemma': [row['lemma']],\n",
    "                   'word_form': [row['word_form']],\n",
    "                   'tokenized_sentence': [inputs[\"input_ids\"][0].tolist()],\n",
    "                   'mask_index': [midx[0]]\n",
    "                  }\n",
    "        gather_debug.append(pd.DataFrame(debug_d))\n",
    "\n",
    "\n",
    "    probs_df = pd.concat(gather_df,ignore_index=True)\n",
    "    debug_df = pd.concat(gather_debug,ignore_index=True)\n",
    "    \n",
    "    return probs_df,debug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c976112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=31002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import the necessary\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "### Create the tokenizer and the model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff1d5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db34a950b31344a7bd60a1edca32576f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original list len: 1363 select list len: 1247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c835db89944d8e9b5f4818685f91a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486c0e51bea048fab723e14b77ba99a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original list len: 515 select list len: 508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bde247844a4a3eb6024205fb4a08b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afde4f7e1924859ab5f953150cfd116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original list len: 646 select list len: 627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a05614a42fa4dd9b6ceb15c1bd388ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to grab dataframes containing nouns from the AnCora Tree Banks\n",
    "datapath = 'datasets/'\n",
    "\n",
    "# This line generates 3 dictionaries with hand-annotated noun:article pairs\n",
    "# dct_noun_singletok\n",
    "# dct_noun_multitok_morph\n",
    "# dct_noun_multitok_nonmorph\n",
    "%run -i 'create_annotated_dicts.py'\n",
    "\n",
    "### Load the datasets one by one\n",
    "\n",
    "# For default SINGLE-TOKEN plurals\n",
    "filename = 'nounlist_single-token-plurals.csv'\n",
    "df_noun = pd.read_csv(os.path.join(datapath,filename))\n",
    "dct_noun = dct_noun_singletok\n",
    "df_singletok = make_sentences_df(dct_noun,df_noun)\n",
    "\n",
    "print('original list len:',df_noun.shape[0],'select list len:', len(dct_noun))\n",
    "\n",
    "data_source = 'single-token' \n",
    "probs_df_singletok,_ = get_article_predictions(df_singletok,data_source)\n",
    "\n",
    "\n",
    "# For default MULTI-TOKEN, MORPHEMIC plurals\n",
    "filename = 'nounlist_multi-token-morph-plurals.csv'\n",
    "df_noun = pd.read_csv(os.path.join(datapath,filename))\n",
    "dct_noun = dct_noun_multitok_morph\n",
    "df_multitok_morph = make_sentences_df(dct_noun,df_noun)\n",
    "\n",
    "print('original list len:',df_noun.shape[0],'select list len:', len(dct_noun))\n",
    "\n",
    "data_source = 'morphemic' \n",
    "probs_df_multitok_morph,_ = get_article_predictions(df_multitok_morph,data_source)\n",
    "\n",
    "\n",
    "# For default MULTI-TOKEN, NONMORPHEMIC plurals\n",
    "filename = 'nounlist_multi-token-nonmorph-plurals.csv'\n",
    "df_noun = pd.read_csv(os.path.join(datapath,filename))\n",
    "dct_noun = dct_noun_multitok_nonmorph\n",
    "df_multitok_nonmorph = make_sentences_df(dct_noun,df_noun)\n",
    "\n",
    "print('original list len:',df_noun.shape[0],'select list len:', len(dct_noun))\n",
    "\n",
    "data_source = 'non_morphemic' \n",
    "probs_df_multitok_nonmorph,_ = get_article_predictions(df_multitok_nonmorph,data_source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a2a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df_singletok['surprisal'] = probs_df_singletok['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_morph['surprisal'] = probs_df_multitok_morph['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_nonmorph['surprisal'] = probs_df_multitok_nonmorph['article_probs'].apply(lambda x: -np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737fa84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, morphemic when you expect them to be\n",
    "\n",
    "non_morphemic_list = []\n",
    "for (_,row) in df_multitok_morph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif affix_tokid in plur_tokids:\n",
    "        check_morph = \"morphemic\"\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "        \n",
    "        non_morphemic_list.append(sing)\n",
    "\n",
    "print(non_morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0544c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, nonmorphemic when you expect them to be\n",
    "\n",
    "morphemic_list = []\n",
    "for (_,row) in df_multitok_nonmorph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif (affix_tokid in plur_tokids) & (row['tokenization_type']=='default'):\n",
    "        check_morph = \"morphemic\"\n",
    "        \n",
    "        morphemic_list.append(sing)\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "        \n",
    "print(morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c9dfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save each of these to a dataframe!\n",
    "\n",
    "savepath = 'results_article-agreement/'\n",
    "\n",
    "if ~os.path.exists(savepath): \n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "probs_df_singletok.to_csv(os.path.join(savepath,'results_singletok.csv'))\n",
    "probs_df_multitok_morph.to_csv(os.path.join(savepath,'results_multitok_morph.csv'))\n",
    "probs_df_multitok_nonmorph.to_csv(os.path.join(savepath,'results_multitok_nonmorph.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548544e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
