{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d40805",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/juliakundu/.pyenv/versions/3.7.12/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "### Basic logistical, wrangling, & visualization imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### Model imports\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e8fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define useful functions\n",
    "\n",
    "def make_sentences_df(dct_noun,df_noun):\n",
    "    \"\"\"Creates sentence templates masking what should be a definite plural or singular article\n",
    "    INPUTS: dct_noun: dictionary, noun:article pairs, the article is lowercase, singular, and gendered--this \n",
    "                      was hand-annotated from native Caribbean Spanish speaker intuition and checks in online\n",
    "                      dictionary of the Real Academia EspaÃ±ola; if there are any concerns about gendered article \n",
    "                      errors, check here first\n",
    "            df_noun: pandas dataframe, cols: whole_word (plural), root (becomes our lemma), affix (s or es)--\n",
    "                     this df is drawn from a noun subset of AnCora Tree Bank\"\"\"\n",
    "    \n",
    "    gather_df = []\n",
    "    for noun in tqdm(dct_noun.keys()): \n",
    "        \n",
    "        subdf = df_noun[df_noun['whole_word'] == noun]\n",
    "\n",
    "        ### set up singular & plural template components\n",
    "        sing_art = dct_noun[noun]\n",
    "        sing_art = str(sing_art[0].upper() + sing_art[1])\n",
    "\n",
    "        sing_subj = subdf['root'].values[0]\n",
    "\n",
    "        plur_subj = noun\n",
    "\n",
    "        ### set up target definite articles\n",
    "        if sing_art == 'El':\n",
    "\n",
    "            plur_art= 'Los'\n",
    "\n",
    "\n",
    "        elif sing_art == 'La': \n",
    "\n",
    "            plur_art = 'Las'\n",
    "\n",
    "        ### set up target singluar and plural adjs\n",
    "\n",
    "        sing_adj = \"grande\"\n",
    "\n",
    "        plur_adj = \"grandes\"\n",
    "\n",
    "        ### set up composite template components\n",
    "        affix = subdf['affix'].values[0]\n",
    "        comp_subj = sing_subj +'##'+ affix\n",
    "\n",
    "\n",
    "        ### create all sentence templates\n",
    "\n",
    "        sing_sentence_template = '[MASK] ' + sing_subj + ' [MASK]'\n",
    "\n",
    "        plur_sentence_template = '[MASK] ' + plur_subj + ' [MASK]'\n",
    "\n",
    "        comp_sentence_template = '[MASK] ' + comp_subj + ' [MASK]'\n",
    "\n",
    "        ### set up all the column/row entries you will store per lemma\n",
    "\n",
    "        all_wordforms = [sing_subj,\n",
    "                         plur_subj,\n",
    "                         comp_subj]\n",
    "\n",
    "        all_word_number = ['sing',\n",
    "                           'plur',\n",
    "                           'plur'] \n",
    "\n",
    "        all_n_tokens = [len(tokenizer.encode(sing_subj,add_special_tokens=False)),\n",
    "                        len(tokenizer.encode(plur_subj,add_special_tokens=False)),\n",
    "                        len(tokenizer.encode(sing_subj,add_special_tokens=False)) + len(tokenizer.encode(['##'+affix],add_special_tokens=False))                           ]\n",
    "\n",
    "        all_sentence_templates = [sing_sentence_template,\n",
    "                                  plur_sentence_template,\n",
    "                                  comp_sentence_template]\n",
    "            \n",
    "        #this is a super important column! distinguishes the default tokenization from \n",
    "        #our artificially imposed tokenization scheme\n",
    "        all_tokenization_types = ['default',\n",
    "                                  'default',\n",
    "                                  'artificial']\n",
    "\n",
    "        d = {'lemma': np.repeat(sing_subj,len(all_sentence_templates)),\n",
    "             'word_form': all_wordforms,\n",
    "             'word_number': all_word_number,\n",
    "             'n_tokens': all_n_tokens,\n",
    "             'tokenization_type': all_tokenization_types,\n",
    "             'sentence': all_sentence_templates,\n",
    "             'target_ART_sing': sing_art, \n",
    "             'target_ART_plur': plur_art,\n",
    "             'target_adj_sing': sing_adj,\n",
    "             'target_adj_plur': plur_adj,\n",
    "             'affix': np.repeat(affix,len(all_sentence_templates))\n",
    "             }\n",
    "\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "    sentence_df = pd.concat(gather_df,ignore_index=True)\n",
    "    return sentence_df\n",
    "\n",
    "\n",
    "### Model Predictions \n",
    "\n",
    "def find_sublist_index(list, sublist):\n",
    "    \"\"\"Find the first occurence of sublist in list.\n",
    "    Return the start and end indices of sublist in list.\n",
    "    Used to find index of [MASK] in template sentences.\n",
    "\n",
    "    h/t GPT-3-codex for writing this.\"\"\"\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == sublist[0] and list[i:i+len(sublist)] == sublist:\n",
    "            count += 1\n",
    "        if count == 2:\n",
    "            return i, i+len(sublist)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_article_predictions(df,data_source): \n",
    "    \"\"\"Predict the likelihood of target definite/indefinite and singular/plural articles\n",
    "       Will assume you've already loaded and defined your tokenizer and model.\n",
    "       Will iterate row by row in your dataframe `df`, containing cols for masked sentences,\n",
    "       the corresponding lemma and plural forms being tested. When it comes across a row\n",
    "       with column `tokenizer_type` label `artificial`, will shunt you to a process that creates\n",
    "       the inputs to the model by hand (if there are any issues/concerns with how the artificial\n",
    "       tokenization proceeds and leads to predictions, check the following `if` statement: \n",
    "       `if row['tokenization_type'] == 'artificial'`)\n",
    "       \n",
    "       INPUTS: df, pandas dataframe, cols for lemma, word_number (plural, singular), tokenization_type\n",
    "                   (artificial/default), masked sentence, target singular and plural articles to \n",
    "                   get probabilities for filling in the [MASK], and others\"\"\"\n",
    "\n",
    "    gather_df = []\n",
    "    gather_debug = []\n",
    "    for (_,row) in tqdm(df.iterrows(),total=df.shape[0]):\n",
    "\n",
    "        target_article_sing = row['target_ART_sing']\n",
    "        target_article_plur = row['target_ART_plur']\n",
    "\n",
    "\n",
    "        #tokens for each article type \n",
    "        token_article_sing = tokenizer.encode(target_article_sing,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        token_article_plur = tokenizer.encode(target_article_plur,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        #token for mask\n",
    "        token_mask = tokenizer.encode('[MASK]',\n",
    "                                      add_special_tokens=False\n",
    "                                     )\n",
    "\n",
    "        target_adj_sing = row['target_adj_sing']\n",
    "        target_adj_plur = row['target_adj_plur']\n",
    "\n",
    "        #tokens for each adj number \n",
    "        token_adj_sing = tokenizer.encode(target_adj_sing,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        token_adj_plur = tokenizer.encode(target_adj_plur,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "\n",
    "        ### Set up your representation of the sentence for the \n",
    "        #.  model\n",
    "\n",
    "        if row['tokenization_type'] == 'artificial': \n",
    "\n",
    "            if '##es' in row['word_form']:\n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##es\"])\n",
    "\n",
    "\n",
    "            elif '##s' in row['word_form']: \n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##s\"])\n",
    "\n",
    "            #token for singular form\n",
    "            lemma = row['lemma']\n",
    "            token_lemma = tokenizer.encode(lemma,\n",
    "                                           add_special_tokens=False\n",
    "                                          )\n",
    "            print(token_lemma)\n",
    "            #TODO: Check for source and if it's non-morphemic, combine the tokens that the lemma gets broken into\n",
    " \n",
    "            #token for special start\n",
    "            start = '[CLS]'\n",
    "            token_start = tokenizer.encode(start,\n",
    "                                           add_special_tokens=False)\n",
    "            \n",
    "\n",
    "            #token for special end\n",
    "            ending = '[SEP]'\n",
    "            token_end = tokenizer.encode(ending,\n",
    "                                         add_special_tokens=False)\n",
    "            \n",
    "            \n",
    "            ### Collect your tokens into a list that you will then flatten\n",
    "            #   prior to converting to tensor\n",
    "            bulky_token_list = [token_start,\n",
    "                                token_mask,\n",
    "                                token_lemma,\n",
    "                                token_affix,\n",
    "                                token_mask,\n",
    "                                token_end\n",
    "                                ]\n",
    "            flat_token_list = [item for sublist in bulky_token_list for item in sublist]\n",
    "            token_idx = torch.tensor([flat_token_list])\n",
    "            \n",
    "\n",
    "            inputs = {'input_ids': token_idx,\n",
    "                      'token_type_ids': torch.zeros_like(token_idx),\n",
    "                      'attention_mask': torch.ones_like(token_idx)\n",
    "                     }\n",
    "\n",
    "        elif row['tokenization_type']=='default': \n",
    "\n",
    "            inputs = tokenizer(row['sentence'],\n",
    "                               return_tensors='pt',\n",
    "                               add_special_tokens=True\n",
    "                              )\n",
    "            \n",
    "        model_token_inputs = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0])\n",
    "        model_token_inputs = ' , '.join(model_token_inputs)\n",
    "\n",
    "        ### Predict the item that should fill in the mask!\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        #find the index of the mask in sentence\n",
    "        midx = find_sublist_index(inputs[\"input_ids\"][0].tolist(),\n",
    "                                  token_mask)\n",
    "                                 \n",
    "        masked_token_logits = outputs.logits[0][midx[0]]\n",
    "        masked_token_probs = torch.softmax(masked_token_logits, dim=0)\n",
    "\n",
    "\n",
    "        # prob_article_sing = masked_token_probs[token_article_sing].item()\n",
    "        # prob_article_plur = masked_token_probs[token_article_plur].item()\n",
    "\n",
    "        # prob_list = [prob_article_sing, prob_article_plur]\n",
    "        # article_list = [target_article_sing, target_article_plur]\n",
    "    \n",
    "\n",
    "        prob_adj_sing = masked_token_probs[token_adj_sing].item()\n",
    "        prob_adj_plur = masked_token_probs[token_adj_plur].item()\n",
    "\n",
    "        prob_list = [prob_adj_sing, prob_adj_plur]\n",
    "        adj_list = [target_adj_sing, target_adj_plur]\n",
    "\n",
    "    \n",
    "        ### Store your results\n",
    "\n",
    "        d = {'lemma': np.repeat(row['lemma'],len(prob_list)),\n",
    "             'word_form': np.repeat(row['word_form'],len(prob_list)),\n",
    "             'word_number': np.repeat(row['word_number'],len(prob_list)),\n",
    "             'n_tokens': np.repeat(row['n_tokens'],len(prob_list)),\n",
    "             'tokenization_type': np.repeat(row['tokenization_type'],len(prob_list)),\n",
    "             'adj_probs': prob_list,\n",
    "             'article_number': ['singular','plural'],\n",
    "             'adj_number': ['singular','plural'],\n",
    "             'adj': adj_list,\n",
    "             'affix': row['affix'],\n",
    "             'sentence': row['sentence'],\n",
    "             'model_token_inputs': model_token_inputs,\n",
    "             'source': data_source\n",
    "            }\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "\n",
    "        debug_d = {'lemma': [row['lemma']],\n",
    "                   'word_form': [row['word_form']],\n",
    "                   'tokenized_sentence': [inputs[\"input_ids\"][0].tolist()],\n",
    "                   'mask_index': [midx[0]]\n",
    "                  }\n",
    "        gather_debug.append(pd.DataFrame(debug_d))\n",
    "\n",
    "\n",
    "    probs_df = pd.concat(gather_df,ignore_index=True)\n",
    "    debug_df = pd.concat(gather_debug,ignore_index=True)\n",
    "    \n",
    "    return probs_df,debug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c976112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=31002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import the necessary\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "### Create the tokenizer and the model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff1d5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6578109c696c49c5a5d6848277e7be9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original list len: 1363 select list len: 1247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614d2534f900432faad78d75a7a38944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5850]\n",
      "[14089]\n",
      "[7388]\n",
      "[5911]\n",
      "[9823]\n",
      "[3785]\n",
      "[2378]\n",
      "[13651]\n",
      "[17022]\n",
      "[1944]\n",
      "[14878]\n",
      "[4772]\n",
      "[7746]\n",
      "[2274]\n",
      "[5411]\n",
      "[3921]\n",
      "[2784]\n",
      "[2957]\n",
      "[7043]\n",
      "[2395]\n",
      "[5721]\n",
      "[2966]\n",
      "[1814]\n",
      "[3325]\n",
      "[4443]\n",
      "[2116]\n",
      "[12032]\n",
      "[4149]\n",
      "[3455]\n",
      "[2209]\n",
      "[11381]\n",
      "[12653]\n",
      "[5444]\n",
      "[7473]\n",
      "[1890]\n",
      "[6560]\n",
      "[8222]\n",
      "[7076]\n",
      "[5186]\n",
      "[4627]\n",
      "[8316]\n",
      "[4598]\n",
      "[4379]\n",
      "[27438]\n",
      "[15018]\n",
      "[6013]\n",
      "[2709]\n",
      "[4244]\n",
      "[2307]\n",
      "[5651]\n",
      "[12500]\n",
      "[3179]\n",
      "[14955]\n",
      "[2981]\n",
      "[22553]\n",
      "[11623]\n",
      "[9200]\n",
      "[5070]\n",
      "[4840]\n",
      "[1730]\n",
      "[6835]\n",
      "[1560]\n",
      "[7172]\n",
      "[3434]\n",
      "[9454]\n",
      "[9342]\n",
      "[6114]\n",
      "[4660]\n",
      "[11171]\n",
      "[15643]\n",
      "[3305]\n",
      "[1708]\n",
      "[8955]\n",
      "[3219]\n",
      "[7984]\n",
      "[4436]\n",
      "[5705]\n",
      "[3918]\n",
      "[3630]\n",
      "[2353]\n",
      "[6067]\n",
      "[9866]\n",
      "[2483]\n",
      "[7958]\n",
      "[5029]\n",
      "[4792]\n",
      "[12078]\n",
      "[6289]\n",
      "[5529]\n",
      "[24137]\n",
      "[2153]\n",
      "[3038]\n",
      "[10884]\n",
      "[12112]\n",
      "[9574]\n",
      "[3531]\n",
      "[4196]\n",
      "[10508]\n",
      "[11554]\n",
      "[2984]\n",
      "[15951]\n",
      "[2743]\n",
      "[5076]\n",
      "[6599]\n",
      "[14627]\n",
      "[3889]\n",
      "[2444]\n",
      "[18803]\n",
      "[4198]\n",
      "[4837]\n",
      "[6876]\n",
      "[12175]\n",
      "[3599]\n",
      "[3428]\n",
      "[1726]\n",
      "[27125]\n",
      "[9291]\n",
      "[6290]\n",
      "[3661]\n",
      "[5161]\n",
      "[12049]\n",
      "[9738]\n",
      "[4137]\n",
      "[7412]\n",
      "[11551]\n",
      "[15817]\n",
      "[17802]\n",
      "[2785]\n",
      "[7484]\n",
      "[3740]\n",
      "[9712]\n",
      "[13820]\n",
      "[2181]\n",
      "[5247]\n",
      "[3912]\n",
      "[3165]\n",
      "[4320]\n",
      "[30563]\n",
      "[6362]\n",
      "[1785]\n",
      "[3043]\n",
      "[14412]\n",
      "[7485]\n",
      "[2926]\n",
      "[11211]\n",
      "[1900]\n",
      "[4271]\n",
      "[20795]\n",
      "[1729]\n",
      "[7618]\n",
      "[12184]\n",
      "[12105]\n",
      "[4020]\n",
      "[18855]\n",
      "[4549]\n",
      "[1565]\n",
      "[2606]\n",
      "[19791]\n",
      "[6946]\n",
      "[16991]\n",
      "[3487]\n",
      "[14210]\n",
      "[3783]\n",
      "[4620]\n",
      "[8692]\n",
      "[15756]\n",
      "[4073]\n",
      "[5085]\n",
      "[7901]\n",
      "[19485]\n",
      "[2962]\n",
      "[4412]\n",
      "[20444]\n",
      "[10996]\n",
      "[3880]\n",
      "[7153]\n",
      "[9696]\n",
      "[4053]\n",
      "[4404]\n",
      "[5777]\n",
      "[9088]\n",
      "[20776]\n",
      "[8905]\n",
      "[4675]\n",
      "[4425]\n",
      "[6624]\n",
      "[3134]\n",
      "[5531]\n",
      "[8961]\n",
      "[6447]\n",
      "[2714]\n",
      "[6910]\n",
      "[6278]\n",
      "[12628]\n",
      "[14785]\n",
      "[11023]\n",
      "[20502]\n",
      "[5162]\n",
      "[4980]\n",
      "[4272]\n",
      "[3343]\n",
      "[9961]\n",
      "[5164]\n",
      "[3743]\n",
      "[5535]\n",
      "[25428]\n",
      "[18446]\n",
      "[2467]\n",
      "[12241]\n",
      "[5392]\n",
      "[2860]\n",
      "[26523]\n",
      "[21263]\n",
      "[6382]\n",
      "[3236]\n",
      "[21868]\n",
      "[5508]\n",
      "[11840]\n",
      "[7159]\n",
      "[5400]\n",
      "[20149]\n",
      "[5533]\n",
      "[16734]\n",
      "[9403]\n",
      "[12527]\n",
      "[6298]\n",
      "[9949]\n",
      "[4190]\n",
      "[16491]\n",
      "[5550]\n",
      "[3640]\n",
      "[16559]\n",
      "[5254]\n",
      "[2492]\n",
      "[4355]\n",
      "[10238]\n",
      "[20481]\n",
      "[5697]\n",
      "[2320]\n",
      "[11707]\n",
      "[1802]\n",
      "[6846]\n",
      "[8079]\n",
      "[3773]\n",
      "[3433]\n",
      "[2410]\n",
      "[5562]\n",
      "[3040]\n",
      "[7673]\n",
      "[4997]\n",
      "[7140]\n",
      "[7915]\n",
      "[1642]\n",
      "[6322]\n",
      "[3330]\n",
      "[2341]\n",
      "[8873]\n",
      "[4029]\n",
      "[6324]\n",
      "[3551]\n",
      "[5585]\n",
      "[17938]\n",
      "[4878]\n",
      "[1577]\n",
      "[6870]\n",
      "[4050]\n",
      "[5983]\n",
      "[13118]\n",
      "[23509]\n",
      "[11021]\n",
      "[8814]\n",
      "[7112]\n",
      "[9693]\n",
      "[2199]\n",
      "[1903]\n",
      "[15996]\n",
      "[4868]\n",
      "[6388]\n",
      "[7253]\n",
      "[15740]\n",
      "[3850]\n",
      "[8376]\n",
      "[6714]\n",
      "[25947]\n",
      "[12079]\n",
      "[3756]\n",
      "[7367]\n",
      "[2557]\n",
      "[3484]\n",
      "[7320]\n",
      "[11333]\n",
      "[16518]\n",
      "[2653]\n",
      "[3929]\n",
      "[4811]\n",
      "[13712]\n",
      "[5075]\n",
      "[8323]\n",
      "[11864]\n",
      "[14568]\n",
      "[19557]\n",
      "[1688]\n",
      "[3169]\n",
      "[5895]\n",
      "[3854]\n",
      "[8168]\n",
      "[2114]\n",
      "[2757]\n",
      "[17356]\n",
      "[13182]\n",
      "[3312]\n",
      "[9360]\n",
      "[4376]\n",
      "[4366]\n",
      "[5744]\n",
      "[3872]\n",
      "[9303]\n",
      "[6627]\n",
      "[4354]\n",
      "[19929]\n",
      "[15842]\n",
      "[3264]\n",
      "[5564]\n",
      "[7122]\n",
      "[8554]\n",
      "[3422]\n",
      "[5122]\n",
      "[5499]\n",
      "[4422]\n",
      "[13332]\n",
      "[8346]\n",
      "[2551]\n",
      "[8769]\n",
      "[11466]\n",
      "[3742]\n",
      "[8223]\n",
      "[10901]\n",
      "[17060]\n",
      "[23326]\n",
      "[23049]\n",
      "[14782]\n",
      "[6845]\n",
      "[5670]\n",
      "[2109]\n",
      "[3955]\n",
      "[16677]\n",
      "[4104]\n",
      "[19206]\n",
      "[1675]\n",
      "[1200]\n",
      "[7248]\n",
      "[4041]\n",
      "[2591]\n",
      "[6232]\n",
      "[12765]\n",
      "[9201]\n",
      "[2844]\n",
      "[4052]\n",
      "[6737]\n",
      "[6842]\n",
      "[4210]\n",
      "[15369]\n",
      "[1952]\n",
      "[11783]\n",
      "[10089]\n",
      "[17070]\n",
      "[7646]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal list len:\u001b[39m\u001b[38;5;124m'\u001b[39m,df_noun\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect list len:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dct_noun))\n\u001b[1;32m     20\u001b[0m data_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle-token\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m---> 21\u001b[0m probs_df_singletok,_ \u001b[38;5;241m=\u001b[39m get_article_predictions(df_singletok,data_source)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# For default MULTI-TOKEN, MORPHEMIC plurals\u001b[39;00m\n\u001b[1;32m     25\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnounlist_multi-token-morph-plurals.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 224\u001b[0m, in \u001b[0;36mget_article_predictions\u001b[0;34m(df, data_source)\u001b[0m\n\u001b[1;32m    220\u001b[0m model_token_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m , \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(model_token_inputs)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m### Predict the item that should fill in the mask!\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m#find the index of the mask in sentence\u001b[39;00m\n\u001b[1;32m    227\u001b[0m midx \u001b[38;5;241m=\u001b[39m find_sublist_index(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    228\u001b[0m                           token_mask)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1492\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1492\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m   1493\u001b[0m     input_ids,\n\u001b[1;32m   1494\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1495\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1496\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1497\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1498\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1499\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1500\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1501\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1502\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1503\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1504\u001b[0m )\n\u001b[1;32m   1506\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1507\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1143\u001b[0m     embedding_output,\n\u001b[1;32m   1144\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1145\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1146\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1147\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1148\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1149\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1150\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1151\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    696\u001b[0m         hidden_states,\n\u001b[1;32m    697\u001b[0m         attention_mask,\n\u001b[1;32m    698\u001b[0m         layer_head_mask,\n\u001b[1;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    701\u001b[0m         past_key_value,\n\u001b[1;32m    702\u001b[0m         output_attentions,\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    586\u001b[0m         hidden_states,\n\u001b[1;32m    587\u001b[0m         attention_mask,\n\u001b[1;32m    588\u001b[0m         head_mask,\n\u001b[1;32m    589\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    590\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    516\u001b[0m         hidden_states,\n\u001b[1;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m    518\u001b[0m         head_mask,\n\u001b[1;32m    519\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    520\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    521\u001b[0m         past_key_value,\n\u001b[1;32m    522\u001b[0m         output_attentions,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:408\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    406\u001b[0m     key_layer, value_layer \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[1;32m    409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path to grab dataframes containing nouns from the AnCora Tree Banks\n",
    "datapath = 'datasets/'\n",
    "\n",
    "# This line generates 3 dictionaries with hand-annotated noun:article pairs\n",
    "# dct_noun_singletok\n",
    "# dct_noun_multitok_morph\n",
    "# dct_noun_multitok_nonmorph\n",
    "%run -i 'create_annotated_dicts.py'\n",
    "\n",
    "### Load the datasets one by one\n",
    "\n",
    "# For default SINGLE-TOKEN plurals\n",
    "filename = 'nounlist_single-token-plurals.csv'\n",
    "df_noun = pd.read_csv(os.path.join(datapath,filename))\n",
    "dct_noun = dct_noun_singletok\n",
    "df_singletok = make_sentences_df(dct_noun,df_noun)\n",
    "\n",
    "print('original list len:',df_noun.shape[0],'select list len:', len(dct_noun))\n",
    "\n",
    "data_source = 'single-token' \n",
    "probs_df_singletok,_ = get_article_predictions(df_singletok,data_source)\n",
    "\n",
    "\n",
    "# For default MULTI-TOKEN, MORPHEMIC plurals\n",
    "filename = 'nounlist_multi-token-morph-plurals.csv'\n",
    "df_noun = pd.read_csv(os.path.join(datapath,filename))\n",
    "dct_noun = dct_noun_multitok_morph\n",
    "df_multitok_morph = make_sentences_df(dct_noun,df_noun)\n",
    "\n",
    "print('original list len:',df_noun.shape[0],'select list len:', len(dct_noun))\n",
    "\n",
    "data_source = 'morphemic' \n",
    "probs_df_multitok_morph,_ = get_article_predictions(df_multitok_morph,data_source)\n",
    "\n",
    "\n",
    "# For default MULTI-TOKEN, NONMORPHEMIC plurals\n",
    "filename = 'nounlist_multi-token-nonmorph-plurals.csv'\n",
    "df_noun = pd.read_csv(os.path.join(datapath,filename))\n",
    "dct_noun = dct_noun_multitok_nonmorph\n",
    "df_multitok_nonmorph = make_sentences_df(dct_noun,df_noun)\n",
    "\n",
    "print('original list len:',df_noun.shape[0],'select list len:', len(dct_noun))\n",
    "\n",
    "data_source = 'non_morphemic' \n",
    "probs_df_multitok_nonmorph,_ = get_article_predictions(df_multitok_nonmorph,data_source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a2a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df_singletok['surprisal'] = probs_df_singletok['adj_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_morph['surprisal'] = probs_df_multitok_morph['adj_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_nonmorph['surprisal'] = probs_df_multitok_nonmorph['adj_probs'].apply(lambda x: -np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737fa84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, morphemic when you expect them to be\n",
    "\n",
    "non_morphemic_list = []\n",
    "for (_,row) in df_multitok_morph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif affix_tokid in plur_tokids:\n",
    "        check_morph = \"morphemic\"\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "        \n",
    "        non_morphemic_list.append(sing)\n",
    "\n",
    "print(non_morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0544c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, nonmorphemic when you expect them to be\n",
    "\n",
    "morphemic_list = []\n",
    "for (_,row) in df_multitok_nonmorph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif (affix_tokid in plur_tokids) & (row['tokenization_type']=='default'):\n",
    "        check_morph = \"morphemic\"\n",
    "        \n",
    "        morphemic_list.append(sing)\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "        \n",
    "print(morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9dfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save each of these to a dataframe!\n",
    "\n",
    "savepath = 'results_adj-agreement/'\n",
    "\n",
    "# # if ~os.path.exists(savepath): \n",
    "os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "probs_df_singletok.to_csv(os.path.join(savepath,'results_singletok.csv'))\n",
    "probs_df_multitok_morph.to_csv(os.path.join(savepath,'results_multitok_morph.csv'))\n",
    "probs_df_multitok_nonmorph.to_csv(os.path.join(savepath,'results_multitok_nonmorph.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548544e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
