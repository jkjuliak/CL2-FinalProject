{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from create_annotated_dicts import dct_noun_singletok, dct_noun_multitok_morph, dct_noun_multitok_nonmorph\n",
    "\n",
    "def introduce_typo(word):\n",
    "    \"\"\"Replaces a random letter in the word with a random letter, resembling a typo.\"\"\"\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    original_char = word[idx]\n",
    "    spanish_letters = list('abcdefghijklmnñopqrstuvwxyz')\n",
    "    if original_char in spanish_letters:\n",
    "        letters = spanish_letters.copy()\n",
    "        letters.remove(original_char)\n",
    "        random_letter = random.choice(letters)\n",
    "        mod_word = word[:idx] + random_letter + word[idx + 1:]\n",
    "        return mod_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "noisy_singletok = {introduce_typo(noun): article for noun, article in dct_noun_singletok.items()}\n",
    "noisy_morph = {introduce_typo(noun): article for noun, article in dct_noun_multitok_morph.items()}\n",
    "noisy_nonmorph = {introduce_typo(noun): article for noun, article in dct_noun_multitok_nonmorph.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define useful functions\n",
    "\n",
    "def make_sentences_df(dct_noun,df_noun):\n",
    "    \"\"\"Creates sentence templates masking what should be a definite or indefinite, plural or singular article\n",
    "    INPUTS: dct_noun: dictionary, noun:article pairs, the article is lowercase, singular, and gendered--this \n",
    "                      was hand-annotated from native Caribbean Spanish speaker intuition and checks in online\n",
    "                      dictionary of the Real Academia Española; if there are any concerns about gendered article \n",
    "                      errors, check here first\n",
    "            df_noun: pandas dataframe, cols: whole_word (plural), root (becomes our lemma), affix (s or es)--\n",
    "                     this df is drawn from a noun subset of AnCora Tree Bank\"\"\"\n",
    "    \n",
    "    gather_df = []\n",
    "    for noun in tqdm(dct_noun.keys()): \n",
    "        \n",
    "        #typo introduced\n",
    "        subdf = df_noun[df_noun['whole_word'] == introduce_typo(noun)]\n",
    "\n",
    "        ### set up singular & plural template components\n",
    "        sing_art_def = dct_noun[noun]\n",
    "        sing_art_def = str(sing_art_def[0].upper() + sing_art_def[1])\n",
    "\n",
    "        sing_subj = subdf['root'].values[0]\n",
    "\n",
    "        plur_subj = noun\n",
    "\n",
    "        ### set up target definite and indefinite articles\n",
    "        if sing_art_def == 'El':\n",
    "\n",
    "            plur_art_def = 'Los'\n",
    "            plur_art_indef = 'Unos'\n",
    "\n",
    "            sing_art_indef = 'Un'\n",
    "\n",
    "\n",
    "        elif sing_art_def == 'La': \n",
    "\n",
    "            plur_art_def = 'Las'\n",
    "            plur_art_indef = 'Unas'\n",
    "\n",
    "            sing_art_indef = 'Una'\n",
    "\n",
    "\n",
    "        ### set up composite template components\n",
    "        affix = subdf['affix'].values[0]\n",
    "        comp_subj = sing_subj +'##'+ affix\n",
    "\n",
    "\n",
    "        ### create all sentence templates\n",
    "        article_types = ['definite','indefinite']\n",
    "        for article_type in article_types: \n",
    "\n",
    "            sing_sentence_template = '[MASK] ' + sing_subj \n",
    "\n",
    "            plur_sentence_template = '[MASK] ' + plur_subj \n",
    "\n",
    "            comp_sentence_template = '[MASK] ' + comp_subj \n",
    "\n",
    "\n",
    "            ### set up all the column/row entries you will store per lemma\n",
    "\n",
    "            all_wordforms = [sing_subj,\n",
    "                             plur_subj,\n",
    "                             comp_subj]\n",
    "\n",
    "            all_word_number = ['sing',\n",
    "                               'plur',\n",
    "                               'plur'] \n",
    "\n",
    "            all_n_tokens = [len(tokenizer.encode(sing_subj,add_special_tokens=False)),\n",
    "                            len(tokenizer.encode(plur_subj,add_special_tokens=False)),\n",
    "                            len(tokenizer.encode(sing_subj,add_special_tokens=False)) + len(tokenizer.encode(['##'+affix],add_special_tokens=False))\n",
    "                           ]\n",
    "\n",
    "            all_sentence_templates = [sing_sentence_template,\n",
    "                                      plur_sentence_template,\n",
    "                                      comp_sentence_template]\n",
    "            \n",
    "            #this is a super important column! distinguishes the default tokenization from \n",
    "            #our artificially imposed tokenization scheme\n",
    "            all_tokenization_types = ['default',\n",
    "                                      'default',\n",
    "                                      'artificial'\n",
    "                                     ]\n",
    "\n",
    "            if article_type == 'definite': \n",
    "                sing_art = sing_art_def\n",
    "                plur_art = plur_art_def\n",
    "\n",
    "            elif article_type == 'indefinite': \n",
    "                sing_art = sing_art_indef\n",
    "                plur_art = plur_art_indef\n",
    "\n",
    "            d = {'lemma': np.repeat(sing_subj,len(all_sentence_templates)),\n",
    "                 'word_form': all_wordforms,\n",
    "                 'word_number': all_word_number,\n",
    "                 'n_tokens': all_n_tokens,\n",
    "                 'tokenization_type': all_tokenization_types,\n",
    "                 'sentence': all_sentence_templates,\n",
    "                 'target_ART_sing': sing_art,\n",
    "                 'target_ART_plur': plur_art,\n",
    "                 'article_type': article_type,\n",
    "                 'affix': np.repeat(affix,len(all_sentence_templates))\n",
    "                }\n",
    "\n",
    "            gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "    sentence_df = pd.concat(gather_df,ignore_index=True)\n",
    "    return sentence_df\n",
    "\n",
    "\n",
    "### Model Predictions \n",
    "\n",
    "def find_sublist_index(list, sublist):\n",
    "    \"\"\"Find the first occurence of sublist in list.\n",
    "    Return the start and end indices of sublist in list.\n",
    "    Used to find index of [MASK] in template sentences.\n",
    "\n",
    "    h/t GPT-3-codex for writing this.\"\"\"\n",
    "\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == sublist[0] and list[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_article_predictions(df,data_source): \n",
    "    \"\"\"Predict the likelihood of target definite/indefinite and singular/plural articles\n",
    "       Will assume you've already loaded and defined your tokenizer and model.\n",
    "       Will iterate row by row in your dataframe `df`, containing cols for masked sentences,\n",
    "       the corresponding lemma and plural forms being tested. When it comes across a row\n",
    "       with column `tokenizer_type` label `artificial`, will shunt you to a process that creates\n",
    "       the inputs to the model by hand (if there are any issues/concerns with how the artificial\n",
    "       tokenization proceeds and leads to predictions, check the following `if` statement: \n",
    "       `if row['tokenization_type'] == 'artificial'`)\n",
    "       \n",
    "       INPUTS: df, pandas dataframe, cols for lemma, word_number (plural, singular), tokenization_type\n",
    "                   (artificial/default), masked sentence, target singular and plural articles to \n",
    "                   get probabilities for filling in the [MASK], and others\"\"\"\n",
    "\n",
    "    gather_df = []\n",
    "    gather_debug = []\n",
    "    for (_,row) in tqdm(df.iterrows(),total=df.shape[0]):\n",
    "\n",
    "        target_article_sing = row['target_ART_sing']\n",
    "        target_article_plur = row['target_ART_plur']\n",
    "\n",
    "\n",
    "        #tokens for each article type \n",
    "        token_article_sing = tokenizer.encode(target_article_sing,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        token_article_plur = tokenizer.encode(target_article_plur,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        #token for mask\n",
    "        token_mask = tokenizer.encode('[MASK]',\n",
    "                                      add_special_tokens=False\n",
    "                                     )\n",
    "\n",
    "\n",
    "        ### Set up your representation of the sentence for the \n",
    "        #.  model\n",
    "\n",
    "        if row['tokenization_type'] == 'artificial': \n",
    "\n",
    "            if '##es' in row['word_form']:\n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##es\"])\n",
    "\n",
    "\n",
    "            elif '##s' in row['word_form']: \n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##s\"])\n",
    "\n",
    "\n",
    "            #token for singular form\n",
    "            lemma = row['lemma']\n",
    "            token_lemma = tokenizer.encode(lemma,\n",
    "                                           add_special_tokens=False\n",
    "                                          )\n",
    "            #TODO: How to combine the two tokens into one if the source is the non-morphemic plurals\n",
    " \n",
    "            #token for special start\n",
    "            start = '[CLS]'\n",
    "            token_start = tokenizer.encode(start,\n",
    "                                           add_special_tokens=False)\n",
    "            \n",
    "\n",
    "            #token for special end\n",
    "            ending = '[SEP]'\n",
    "            token_end = tokenizer.encode(ending,\n",
    "                                         add_special_tokens=False)\n",
    "            \n",
    "            \n",
    "            ### Collect your tokens into a list that you will then flatten\n",
    "            #   prior to converting to tensor\n",
    "            bulky_token_list = [token_start,\n",
    "                                token_mask,\n",
    "                                token_lemma,\n",
    "                                token_affix,\n",
    "                                token_end\n",
    "                                ]\n",
    "            flat_token_list = [item for sublist in bulky_token_list for item in sublist]\n",
    "            token_idx = torch.tensor([flat_token_list])\n",
    "            \n",
    "\n",
    "            inputs = {'input_ids': token_idx,\n",
    "                      'token_type_ids': torch.zeros_like(token_idx),\n",
    "                      'attention_mask': torch.ones_like(token_idx)\n",
    "                     }\n",
    "\n",
    "        elif row['tokenization_type']=='default': \n",
    "\n",
    "            inputs = tokenizer(row['sentence'],\n",
    "                               return_tensors='pt',\n",
    "                               add_special_tokens=True\n",
    "                              )\n",
    "            \n",
    "        model_token_inputs = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0])\n",
    "        model_token_inputs = ' , '.join(model_token_inputs)\n",
    "\n",
    "        ### Predict the item that should fill in the mask!\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        #find the index of the mask in sentence\n",
    "        midx = find_sublist_index(inputs[\"input_ids\"][0].tolist(),\n",
    "                                  token_mask)\n",
    "                                 \n",
    "        masked_token_logits = outputs.logits[0][midx[0]]\n",
    "        masked_token_probs = torch.softmax(masked_token_logits, dim=0)\n",
    "\n",
    "\n",
    "        prob_article_sing = masked_token_probs[token_article_sing].item()\n",
    "        prob_article_plur = masked_token_probs[token_article_plur].item()\n",
    "\n",
    "        prob_list = [prob_article_sing, prob_article_plur]\n",
    "        article_list = [target_article_sing, target_article_plur]\n",
    "    \n",
    "\n",
    "    \n",
    "        ### Store your results\n",
    "\n",
    "        d = {'lemma': np.repeat(row['lemma'],len(prob_list)),\n",
    "             'word_form': np.repeat(row['word_form'],len(prob_list)),\n",
    "             'word_number': np.repeat(row['word_number'],len(prob_list)),\n",
    "             'n_tokens': np.repeat(row['n_tokens'],len(prob_list)),\n",
    "             'tokenization_type': np.repeat(row['tokenization_type'],len(prob_list)),\n",
    "             'article_probs': prob_list,\n",
    "             'article_number': ['singular','plural'],\n",
    "             'article_type': np.repeat(row['article_type'],len(prob_list)),\n",
    "             'article': article_list,\n",
    "             'affix': row['affix'],\n",
    "             'sentence': row['sentence'],\n",
    "             'model_token_inputs': model_token_inputs,\n",
    "             'source': data_source\n",
    "            }\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "\n",
    "        debug_d = {'lemma': [row['lemma']],\n",
    "                   'word_form': [row['word_form']],\n",
    "                   'tokenized_sentence': [inputs[\"input_ids\"][0].tolist()],\n",
    "                   'mask_index': [midx[0]]\n",
    "                  }\n",
    "        gather_debug.append(pd.DataFrame(debug_d))\n",
    "\n",
    "\n",
    "    probs_df = pd.concat(gather_df,ignore_index=True)\n",
    "    debug_df = pd.concat(gather_debug,ignore_index=True)\n",
    "    \n",
    "    return probs_df,debug_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(df_noun_s.head())\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dct_noun_s \u001b[38;5;241m=\u001b[39m noisy_singletok\n\u001b[0;32m---> 17\u001b[0m df_singletok \u001b[38;5;241m=\u001b[39m \u001b[43mmake_sentences_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdct_noun_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_noun_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal list len:\u001b[39m\u001b[38;5;124m'\u001b[39m,df_noun_s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect list len:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dct_noun_s))\n\u001b[1;32m     21\u001b[0m data_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle-token\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# CHECK THIS!\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36mmake_sentences_df\u001b[0;34m(dct_noun, df_noun)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates sentence templates masking what should be a definite or indefinite, plural or singular article\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mINPUTS: dct_noun: dictionary, noun:article pairs, the article is lowercase, singular, and gendered--this \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m                  was hand-annotated from native Caribbean Spanish speaker intuition and checks in online\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        df_noun: pandas dataframe, cols: whole_word (plural), root (becomes our lemma), affix (s or es)--\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m                 this df is drawn from a noun subset of AnCora Tree Bank\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m gather_df \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noun \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdct_noun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m: \n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#typo introduced\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     subdf \u001b[38;5;241m=\u001b[39m df_noun[df_noun[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhole_word\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m introduce_typo(noun)]\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m### set up singular & plural template components\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/cl2env/lib/python3.11/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/cl2env/lib/python3.11/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# For default SINGLE-TOKEN plurals\n",
    "\n",
    "df_noun_s = pd.DataFrame(columns=['whole_word', 'root', 'affix'])\n",
    "df_noun_s['whole_word'] = noisy_singletok.keys()\n",
    "s_root_list = []\n",
    "for key in noisy_singletok.keys():\n",
    "    s_root_list.append(key[:-1])\n",
    "s_affix_list = []\n",
    "for i in range(len(s_root_list)):\n",
    "    s_affix_list.append(\"s\")\n",
    "df_noun_s['root'] = s_root_list\n",
    "df_noun_s['affix'] = s_affix_list\n",
    "# print(df_noun_s.head())\n",
    "\n",
    "dct_noun_s = noisy_singletok\n",
    "\n",
    "df_singletok = make_sentences_df(dct_noun_s,df_noun_s)\n",
    "\n",
    "print('original list len:',df_noun_s.shape[0],'select list len:', len(dct_noun_s))\n",
    "\n",
    "data_source = 'single-token' # CHECK THIS!\n",
    "probs_df_singletok,_ = get_article_predictions(df_singletok,data_source)\n",
    "\n",
    "\n",
    "# # For default MULTI-TOKEN, MORPHEMIC plurals\n",
    "\n",
    "df_noun_mm = pd.DataFrame(columns=[\"POS\", \"whole_word\", \"root\", \"affix\"])\n",
    "df_noun_mm['whole_word'] = noisy_morph.keys()\n",
    "mm_root_list = []\n",
    "for key in noisy_morph.keys():\n",
    "    mm_root_list.append(key[:-1])\n",
    "mm_affix_list = []\n",
    "mm_pos_list = []\n",
    "for i in range(len(mm_root_list)):\n",
    "    mm_affix_list.append(\"s\")\n",
    "    mm_pos_list.append(\"NOUN\")\n",
    "df_noun_mm['POS'] = mm_pos_list\n",
    "df_noun_mm['root'] = mm_root_list\n",
    "df_noun_mm['affix'] = mm_affix_list\n",
    "\n",
    "dct_noun_mm = noisy_morph\n",
    "df_multitok_morph = make_sentences_df(dct_noun_mm,df_noun_mm)\n",
    "\n",
    "print('original list len:',df_noun_mm.shape[0],'select list len:', len(dct_noun_mm))\n",
    "\n",
    "data_source = 'morphemic' \n",
    "probs_df_multitok_morph,_ = get_article_predictions(df_multitok_morph,data_source)\n",
    "\n",
    "# # For default MULTI-TOKEN, NONMORPHEMIC plurals\n",
    "\n",
    "df_noun_nm = pd.DataFrame(columns=[\"whole_word\", \"root\", \"affix\", \"tokenized\"])\n",
    "df_noun_nm['whole_word'] = noisy_nonmorph.keys()\n",
    "nm_root_list = []\n",
    "for key in noisy_nonmorph.keys():\n",
    "    nm_root_list.append(key[:-1])\n",
    "nm_affix_list = []\n",
    "nm_tokenized_list = []\n",
    "for i in range(len(nm_root_list)):\n",
    "    word = nm_root_list[i] + \"s\"\n",
    "    nm_affix_list.append(\"s\")\n",
    "    nm_tokenized_list.append(noisy_nonmorph[word])\n",
    "    # mm_pos_list.append(\"NOUN\")\n",
    "df_noun_nm['root'] = nm_root_list\n",
    "df_noun_nm['affix'] = nm_affix_list\n",
    "df_noun_nm['tokenized'] = nm_tokenized_list\n",
    "\n",
    "dct_noun_nm = noisy_nonmorph\n",
    "df_multitok_nonmorph = make_sentences_df(dct_noun_nm,df_noun_nm)\n",
    "\n",
    "print('original list len:',df_noun_nm.shape[0],'select list len:', len(dct_noun_nm))\n",
    "print(df_noun_nm.head())\n",
    "\n",
    "data_source = 'nonmorphemic' \n",
    "probs_df_multitok_nonmorph,_ = get_article_predictions(df_multitok_nonmorph,data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df_singletok['surprisal'] = probs_df_singletok['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_morph['surprisal'] = probs_df_multitok_morph['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_nonmorph['surprisal'] = probs_df_multitok_nonmorph['article_probs'].apply(lambda x: -np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, morphemic when you expect them to be\n",
    "\n",
    "non_morphemic_list = []\n",
    "for (_,row) in df_multitok_morph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif affix_tokid in plur_tokids:\n",
    "        check_morph = \"morphemic\"\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "        \n",
    "        non_morphemic_list.append(sing)\n",
    "\n",
    "print(non_morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, nonmorphemic when you expect them to be\n",
    "\n",
    "morphemic_list = []\n",
    "for (_,row) in df_multitok_nonmorph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif (affix_tokid in plur_tokids) & (row['tokenization_type']=='default'):\n",
    "        check_morph = \"morphemic\"\n",
    "        \n",
    "        morphemic_list.append(sing)\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "        \n",
    "print(morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save each of these to a dataframe!\n",
    "\n",
    "savepath = 'results_noisy_article-agreement/'\n",
    "\n",
    "if not os.path.exists(savepath): \n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "probs_df_singletok.to_csv(os.path.join(savepath,'results_singletok.csv'))\n",
    "probs_df_multitok_morph.to_csv(os.path.join(savepath,'results_multitok_morph.csv'))\n",
    "probs_df_multitok_nonmorph.to_csv(os.path.join(savepath,'results_multitok_nonmorph.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 (pyenv)",
   "language": "python",
   "name": "pyenv-3.11.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
