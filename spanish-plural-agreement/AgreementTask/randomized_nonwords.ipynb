{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define useful functions\n",
    "import random\n",
    "\n",
    "def introduce_typo(word):\n",
    "    \"\"\"Replaces a random letter in the word with a random letter, resembling a typo.\"\"\"\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    original_char = word[idx]\n",
    "    spanish_letters = list('abcdefghijklmnñopqrstuvwxyz')\n",
    "    letters = spanish_letters.copy()\n",
    "    letters.remove(original_char.lower())\n",
    "    random_letter = random.choice(letters)\n",
    "    if original_char.isupper():\n",
    "        random_letter = random_letter.upper()\n",
    "    mod_word = word[:idx] + random_letter + word[idx + 1:]\n",
    "    return mod_word\n",
    "\n",
    "def make_sentences_df(dct_noun,df_noun):\n",
    "    \"\"\"Creates sentence templates masking what should be a definite or indefinite, plural or singular article\n",
    "    INPUTS: dct_noun: dictionary, noun:article pairs, the article is lowercase, singular, and gendered--this \n",
    "                      was hand-annotated from native Caribbean Spanish speaker intuition and checks in online\n",
    "                      dictionary of the Real Academia Española; if there are any concerns about gendered article \n",
    "                      errors, check here first\n",
    "            df_noun: pandas dataframe, cols: whole_word (plural), root (becomes our lemma), affix (s or es)--\n",
    "                     this df is drawn from a noun subset of AnCora Tree Bank\"\"\"\n",
    "    \n",
    "    gather_df = []\n",
    "    for noun in tqdm(dct_noun.keys()): \n",
    "        \n",
    "        #typo introduced\n",
    "        subdf = df_noun[df_noun['whole_word'] == introduce_typo(noun)]\n",
    "\n",
    "        ### set up singular & plural template components\n",
    "        sing_art_def = dct_noun[noun]\n",
    "        sing_art_def = str(sing_art_def[0].upper() + sing_art_def[1])\n",
    "\n",
    "        sing_subj = subdf['root'].values[0]\n",
    "\n",
    "        plur_subj = noun\n",
    "\n",
    "        ### set up target definite and indefinite articles\n",
    "        if sing_art_def == 'El':\n",
    "\n",
    "            plur_art_def = 'Los'\n",
    "            plur_art_indef = 'Unos'\n",
    "\n",
    "            sing_art_indef = 'Un'\n",
    "\n",
    "\n",
    "        elif sing_art_def == 'La': \n",
    "\n",
    "            plur_art_def = 'Las'\n",
    "            plur_art_indef = 'Unas'\n",
    "\n",
    "            sing_art_indef = 'Una'\n",
    "\n",
    "\n",
    "        ### set up composite template components\n",
    "        affix = subdf['affix'].values[0]\n",
    "        comp_subj = sing_subj +'##'+ affix\n",
    "\n",
    "\n",
    "        ### create all sentence templates\n",
    "        article_types = ['definite','indefinite']\n",
    "        for article_type in article_types: \n",
    "\n",
    "            sing_sentence_template = '[MASK] ' + sing_subj \n",
    "\n",
    "            plur_sentence_template = '[MASK] ' + plur_subj \n",
    "\n",
    "            comp_sentence_template = '[MASK] ' + comp_subj \n",
    "\n",
    "\n",
    "            ### set up all the column/row entries you will store per lemma\n",
    "\n",
    "            all_wordforms = [sing_subj,\n",
    "                             plur_subj,\n",
    "                             comp_subj]\n",
    "\n",
    "            all_word_number = ['sing',\n",
    "                               'plur',\n",
    "                               'plur'] \n",
    "\n",
    "            all_n_tokens = [len(tokenizer.encode(sing_subj,add_special_tokens=False)),\n",
    "                            len(tokenizer.encode(plur_subj,add_special_tokens=False)),\n",
    "                            len(tokenizer.encode(sing_subj,add_special_tokens=False)) + len(tokenizer.encode(['##'+affix],add_special_tokens=False))\n",
    "                           ]\n",
    "\n",
    "            all_sentence_templates = [sing_sentence_template,\n",
    "                                      plur_sentence_template,\n",
    "                                      comp_sentence_template]\n",
    "            \n",
    "            #this is a super important column! distinguishes the default tokenization from \n",
    "            #our artificially imposed tokenization scheme\n",
    "            all_tokenization_types = ['default',\n",
    "                                      'default',\n",
    "                                      'artificial'\n",
    "                                     ]\n",
    "\n",
    "            if article_type == 'definite': \n",
    "                sing_art = sing_art_def\n",
    "                plur_art = plur_art_def\n",
    "\n",
    "            elif article_type == 'indefinite': \n",
    "                sing_art = sing_art_indef\n",
    "                plur_art = plur_art_indef\n",
    "\n",
    "            d = {'lemma': np.repeat(sing_subj,len(all_sentence_templates)),\n",
    "                 'word_form': all_wordforms,\n",
    "                 'word_number': all_word_number,\n",
    "                 'n_tokens': all_n_tokens,\n",
    "                 'tokenization_type': all_tokenization_types,\n",
    "                 'sentence': all_sentence_templates,\n",
    "                 'target_ART_sing': sing_art,\n",
    "                 'target_ART_plur': plur_art,\n",
    "                 'article_type': article_type,\n",
    "                 'affix': np.repeat(affix,len(all_sentence_templates))\n",
    "                }\n",
    "\n",
    "            gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "    sentence_df = pd.concat(gather_df,ignore_index=True)\n",
    "    return sentence_df\n",
    "\n",
    "\n",
    "### Model Predictions \n",
    "\n",
    "def find_sublist_index(list, sublist):\n",
    "    \"\"\"Find the first occurence of sublist in list.\n",
    "    Return the start and end indices of sublist in list.\n",
    "    Used to find index of [MASK] in template sentences.\n",
    "\n",
    "    h/t GPT-3-codex for writing this.\"\"\"\n",
    "\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == sublist[0] and list[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_article_predictions(df,data_source): \n",
    "    \"\"\"Predict the likelihood of target definite/indefinite and singular/plural articles\n",
    "       Will assume you've already loaded and defined your tokenizer and model.\n",
    "       Will iterate row by row in your dataframe `df`, containing cols for masked sentences,\n",
    "       the corresponding lemma and plural forms being tested. When it comes across a row\n",
    "       with column `tokenizer_type` label `artificial`, will shunt you to a process that creates\n",
    "       the inputs to the model by hand (if there are any issues/concerns with how the artificial\n",
    "       tokenization proceeds and leads to predictions, check the following `if` statement: \n",
    "       `if row['tokenization_type'] == 'artificial'`)\n",
    "       \n",
    "       INPUTS: df, pandas dataframe, cols for lemma, word_number (plural, singular), tokenization_type\n",
    "                   (artificial/default), masked sentence, target singular and plural articles to \n",
    "                   get probabilities for filling in the [MASK], and others\"\"\"\n",
    "\n",
    "    gather_df = []\n",
    "    gather_debug = []\n",
    "    for (_,row) in tqdm(df.iterrows(),total=df.shape[0]):\n",
    "\n",
    "        target_article_sing = row['target_ART_sing']\n",
    "        target_article_plur = row['target_ART_plur']\n",
    "\n",
    "\n",
    "        #tokens for each article type \n",
    "        token_article_sing = tokenizer.encode(target_article_sing,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        token_article_plur = tokenizer.encode(target_article_plur,\n",
    "                                              add_special_tokens=False\n",
    "                                         )\n",
    "        #token for mask\n",
    "        token_mask = tokenizer.encode('[MASK]',\n",
    "                                      add_special_tokens=False\n",
    "                                     )\n",
    "\n",
    "\n",
    "        ### Set up your representation of the sentence for the \n",
    "        #.  model\n",
    "\n",
    "        if row['tokenization_type'] == 'artificial': \n",
    "\n",
    "            if '##es' in row['word_form']:\n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##es\"])\n",
    "\n",
    "\n",
    "            elif '##s' in row['word_form']: \n",
    "\n",
    "                token_affix = tokenizer.convert_tokens_to_ids([\"##s\"])\n",
    "\n",
    "\n",
    "            #token for singular form\n",
    "            lemma = row['lemma']\n",
    "            token_lemma = tokenizer.encode(lemma,\n",
    "                                           add_special_tokens=False\n",
    "                                          )\n",
    "            #TODO: How to combine the two tokens into one if the source is the non-morphemic plurals\n",
    " \n",
    "            #token for special start\n",
    "            start = '[CLS]'\n",
    "            token_start = tokenizer.encode(start,\n",
    "                                           add_special_tokens=False)\n",
    "            \n",
    "\n",
    "            #token for special end\n",
    "            ending = '[SEP]'\n",
    "            token_end = tokenizer.encode(ending,\n",
    "                                         add_special_tokens=False)\n",
    "            \n",
    "            \n",
    "            ### Collect your tokens into a list that you will then flatten\n",
    "            #   prior to converting to tensor\n",
    "            bulky_token_list = [token_start,\n",
    "                                token_mask,\n",
    "                                token_lemma,\n",
    "                                token_affix,\n",
    "                                token_end\n",
    "                                ]\n",
    "            flat_token_list = [item for sublist in bulky_token_list for item in sublist]\n",
    "            token_idx = torch.tensor([flat_token_list])\n",
    "            \n",
    "\n",
    "            inputs = {'input_ids': token_idx,\n",
    "                      'token_type_ids': torch.zeros_like(token_idx),\n",
    "                      'attention_mask': torch.ones_like(token_idx)\n",
    "                     }\n",
    "\n",
    "        elif row['tokenization_type']=='default': \n",
    "\n",
    "            inputs = tokenizer(row['sentence'],\n",
    "                               return_tensors='pt',\n",
    "                               add_special_tokens=True\n",
    "                              )\n",
    "            \n",
    "        model_token_inputs = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0])\n",
    "        model_token_inputs = ' , '.join(model_token_inputs)\n",
    "\n",
    "        ### Predict the item that should fill in the mask!\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        #find the index of the mask in sentence\n",
    "        midx = find_sublist_index(inputs[\"input_ids\"][0].tolist(),\n",
    "                                  token_mask)\n",
    "                                 \n",
    "        masked_token_logits = outputs.logits[0][midx[0]]\n",
    "        masked_token_probs = torch.softmax(masked_token_logits, dim=0)\n",
    "\n",
    "\n",
    "        prob_article_sing = masked_token_probs[token_article_sing].item()\n",
    "        prob_article_plur = masked_token_probs[token_article_plur].item()\n",
    "\n",
    "        prob_list = [prob_article_sing, prob_article_plur]\n",
    "        article_list = [target_article_sing, target_article_plur]\n",
    "    \n",
    "\n",
    "    \n",
    "        ### Store your results\n",
    "\n",
    "        d = {'lemma': np.repeat(row['lemma'],len(prob_list)),\n",
    "             'word_form': np.repeat(row['word_form'],len(prob_list)),\n",
    "             'word_number': np.repeat(row['word_number'],len(prob_list)),\n",
    "             'n_tokens': np.repeat(row['n_tokens'],len(prob_list)),\n",
    "             'tokenization_type': np.repeat(row['tokenization_type'],len(prob_list)),\n",
    "             'article_probs': prob_list,\n",
    "             'article_number': ['singular','plural'],\n",
    "             'article_type': np.repeat(row['article_type'],len(prob_list)),\n",
    "             'article': article_list,\n",
    "             'affix': row['affix'],\n",
    "             'sentence': row['sentence'],\n",
    "             'model_token_inputs': model_token_inputs,\n",
    "             'source': data_source\n",
    "            }\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "\n",
    "\n",
    "        debug_d = {'lemma': [row['lemma']],\n",
    "                   'word_form': [row['word_form']],\n",
    "                   'tokenized_sentence': [inputs[\"input_ids\"][0].tolist()],\n",
    "                   'mask_index': [midx[0]]\n",
    "                  }\n",
    "        gather_debug.append(pd.DataFrame(debug_d))\n",
    "\n",
    "\n",
    "    probs_df = pd.concat(gather_df,ignore_index=True)\n",
    "    debug_df = pd.concat(gather_debug,ignore_index=True)\n",
    "    \n",
    "    return probs_df,debug_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For default SINGLE-TOKEN plurals\n",
    "\n",
    "df_noun_s = pd.DataFrame(columns=['whole_word', 'root', 'affix'])\n",
    "df_noun_s['whole_word'] = dct_noun_singletok.keys()\n",
    "s_root_list = []\n",
    "for key in dct_noun_singletok.keys():\n",
    "    s_root_list.append(key[:-1])\n",
    "s_affix_list = []\n",
    "for i in range(len(s_root_list)):\n",
    "    s_affix_list.append(\"s\")\n",
    "df_noun_s['root'] = s_root_list\n",
    "df_noun_s['affix'] = s_affix_list\n",
    "# print(df_noun_s.head())\n",
    "\n",
    "dct_noun_s = dct_noun_singletok\n",
    "\n",
    "df_singletok = make_sentences_df(dct_noun_s,df_noun_s)\n",
    "\n",
    "print('original list len:',df_noun_s.shape[0],'select list len:', len(dct_noun_s))\n",
    "\n",
    "data_source = 'single-token' # CHECK THIS!\n",
    "probs_df_singletok,_ = get_article_predictions(df_singletok,data_source)\n",
    "\n",
    "\n",
    "# # For default MULTI-TOKEN, MORPHEMIC plurals\n",
    "\n",
    "df_noun_mm = pd.DataFrame(columns=[\"POS\", \"whole_word\", \"root\", \"affix\"])\n",
    "df_noun_mm['whole_word'] = dct_noun_multi_morph.keys()\n",
    "mm_root_list = []\n",
    "for key in dct_noun_multi_morph.keys():\n",
    "    mm_root_list.append(key[:-1])\n",
    "mm_affix_list = []\n",
    "mm_pos_list = []\n",
    "for i in range(len(mm_root_list)):\n",
    "    mm_affix_list.append(\"s\")\n",
    "    mm_pos_list.append(\"NOUN\")\n",
    "df_noun_mm['POS'] = mm_pos_list\n",
    "df_noun_mm['root'] = mm_root_list\n",
    "df_noun_mm['affix'] = mm_affix_list\n",
    "\n",
    "dct_noun_mm = dct_noun_multi_morph\n",
    "df_multitok_morph = make_sentences_df(dct_noun_mm,df_noun_mm)\n",
    "\n",
    "print('original list len:',df_noun_mm.shape[0],'select list len:', len(dct_noun_mm))\n",
    "\n",
    "data_source = 'morphemic' \n",
    "probs_df_multitok_morph,_ = get_article_predictions(df_multitok_morph,data_source)\n",
    "\n",
    "# # For default MULTI-TOKEN, NONMORPHEMIC plurals\n",
    "\n",
    "df_noun_nm = pd.DataFrame(columns=[\"whole_word\", \"root\", \"affix\", \"tokenized\"])\n",
    "df_noun_nm['whole_word'] = dct_noun_multi_nonmorph.keys()\n",
    "nm_root_list = []\n",
    "for key in dct_noun_multi_nonmorph.keys():\n",
    "    nm_root_list.append(key[:-1])\n",
    "nm_affix_list = []\n",
    "nm_tokenized_list = []\n",
    "for i in range(len(nm_root_list)):\n",
    "    word = nm_root_list[i] + \"s\"\n",
    "    nm_affix_list.append(\"s\")\n",
    "    nm_tokenized_list.append(dct_noun_multi_nonmorph[word])\n",
    "    # mm_pos_list.append(\"NOUN\")\n",
    "df_noun_nm['root'] = nm_root_list\n",
    "df_noun_nm['affix'] = nm_affix_list\n",
    "df_noun_nm['tokenized'] = nm_tokenized_list\n",
    "\n",
    "dct_noun_nm = dct_noun_multi_nonmorph\n",
    "df_multitok_nonmorph = make_sentences_df(dct_noun_nm,df_noun_nm)\n",
    "\n",
    "print('original list len:',df_noun_nm.shape[0],'select list len:', len(dct_noun_nm))\n",
    "print(df_noun_nm.head())\n",
    "\n",
    "data_source = 'nonmorphemic' \n",
    "probs_df_multitok_nonmorph,_ = get_article_predictions(df_multitok_nonmorph,data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df_singletok['surprisal'] = probs_df_singletok['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_morph['surprisal'] = probs_df_multitok_morph['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_nonmorph['surprisal'] = probs_df_multitok_nonmorph['article_probs'].apply(lambda x: -np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, morphemic when you expect them to be\n",
    "\n",
    "non_morphemic_list = []\n",
    "for (_,row) in df_multitok_morph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif affix_tokid in plur_tokids:\n",
    "        check_morph = \"morphemic\"\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "        \n",
    "        non_morphemic_list.append(sing)\n",
    "\n",
    "print(non_morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, nonmorphemic when you expect them to be\n",
    "\n",
    "morphemic_list = []\n",
    "for (_,row) in df_multitok_nonmorph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif (affix_tokid in plur_tokids) & (row['tokenization_type']=='default'):\n",
    "        check_morph = \"morphemic\"\n",
    "        \n",
    "        morphemic_list.append(sing)\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "        \n",
    "print(morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save each of these to a dataframe!\n",
    "\n",
    "savepath = 'results_pseudo_article-agreement/'\n",
    "\n",
    "if not os.path.exists(savepath): \n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "probs_df_singletok.to_csv(os.path.join(savepath,'results_singletok.csv'))\n",
    "probs_df_multitok_morph.to_csv(os.path.join(savepath,'results_multitok_morph.csv'))\n",
    "probs_df_multitok_nonmorph.to_csv(os.path.join(savepath,'results_multitok_nonmorph.csv'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
