{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from create_annotated_dicts import dct_noun_singletok as st, dct_noun_multitok_morph as mm, dct_noun_multitok_nonmorph as nmm\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model.eval()\n",
    "random.seed(42)\n",
    "\n",
    "def introduce_typo(word):\n",
    "    \"\"\"Replaces a random letter in the word with a random letter, resembling a typo.\"\"\"\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    original_char = word[idx]\n",
    "    spanish_letters = list('abcdefghijklmnÃ±opqrstuvwxyz')\n",
    "    if original_char in spanish_letters:\n",
    "        letters = spanish_letters.copy()\n",
    "        letters.remove(original_char)\n",
    "        random_letter = random.choice(letters)\n",
    "        mod_word = word[:idx] + random_letter + word[idx + 1:]\n",
    "        return mod_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "dct_noun_singletok = {introduce_typo(noun): article for noun, article in st.items()}\n",
    "dct_noun_multi_morph = {introduce_typo(noun): article for noun, article in mm.items()}\n",
    "dct_noun_multi_nonmorph = {introduce_typo(noun): article for noun, article in nmm.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1247/1247 [00:00<00:00, 4387.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3741/3741 [02:10<00:00, 28.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 508/508 [00:00<00:00, 4127.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1524/1524 [00:52<00:00, 28.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 627/627 [00:00<00:00, 4051.49it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1881/1881 [01:02<00:00, 30.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "from create_annotated_dicts import dct_noun_singletok as st, dct_noun_multitok_morph as mm, dct_noun_multitok_nonmorph as nmm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model.eval()\n",
    "\n",
    "def introduce_typo(word):\n",
    "    \"\"\"Replaces a random letter in the word with a random letter, resembling a typo.\"\"\"\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    original_char = word[idx]\n",
    "    spanish_letters = list('abcdefghijklmnÃ±opqrstuvwxyz')\n",
    "    if original_char.lower() in spanish_letters:\n",
    "        letters = spanish_letters.copy()\n",
    "        letters.remove(original_char.lower())\n",
    "        random_letter = random.choice(letters)\n",
    "        # Preserve the case of the original character\n",
    "        if original_char.isupper():\n",
    "            random_letter = random_letter.upper()\n",
    "        mod_word = word[:idx] + random_letter + word[idx + 1:]\n",
    "        return mod_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "dct_noun_singletok = st\n",
    "dct_noun_multi_morph = mm\n",
    "dct_noun_multi_nonmorph = nmm\n",
    "\n",
    "def create_dataframe_with_typos(dct_noun):\n",
    "    df_noun = pd.DataFrame({\n",
    "        'whole_word': list(dct_noun.keys())\n",
    "    })\n",
    "    df_noun['whole_word_typo'] = df_noun['whole_word'].apply(introduce_typo)\n",
    "    df_noun['root'] = df_noun['whole_word'].apply(lambda x: x[:-1])  # Assuming plural ends with 's' or 'es'\n",
    "    df_noun['affix'] = df_noun['whole_word'].apply(lambda x: x[-1] if x.endswith('s') else x[-2:])\n",
    "    return df_noun\n",
    "\n",
    "df_noun_s = create_dataframe_with_typos(dct_noun_singletok)\n",
    "df_noun_mm = create_dataframe_with_typos(dct_noun_multi_morph)\n",
    "df_noun_nm = create_dataframe_with_typos(dct_noun_multi_nonmorph)\n",
    "\n",
    "\n",
    "def make_sentences_df(dct_noun, df_noun):\n",
    "    \"\"\"Creates sentence templates masking what should be a definite or indefinite, plural or singular article.\"\"\"\n",
    "    gather_df = []\n",
    "    for idx, row in tqdm(df_noun.iterrows(), total=df_noun.shape[0]): \n",
    "        noun = row['whole_word']\n",
    "        typoed_noun = row['whole_word_typo']\n",
    "        affix = row['affix']\n",
    "        sing_subj = row['root']\n",
    "        sing_art_def = dct_noun[noun]\n",
    "        sing_art_def = str(sing_art_def[0].upper() + sing_art_def[1:])\n",
    "\n",
    "        if sing_art_def == 'El':\n",
    "            plur_art_def = 'Los'\n",
    "            plur_art_indef = 'Unos'\n",
    "            sing_art_indef = 'Un'\n",
    "        elif sing_art_def == 'La': \n",
    "            plur_art_def = 'Las'\n",
    "            plur_art_indef = 'Unas'\n",
    "            sing_art_indef = 'Una'\n",
    "\n",
    "        comp_subj = sing_subj + '##' + affix\n",
    "\n",
    "        article_types = ['definite', 'indefinite']\n",
    "        for article_type in article_types: \n",
    "            sing_sentence_template = '[MASK] ' + sing_subj \n",
    "            plur_sentence_template = '[MASK] ' + typoed_noun  # Use typoed word\n",
    "            comp_sentence_template = '[MASK] ' + comp_subj \n",
    "            all_wordforms = [sing_subj, typoed_noun, comp_subj]\n",
    "            all_word_number = ['sing', 'plur', 'plur']\n",
    "            all_n_tokens = [\n",
    "                len(tokenizer.encode(sing_subj, add_special_tokens=False)),\n",
    "                len(tokenizer.encode(typoed_noun, add_special_tokens=False)),\n",
    "                len(tokenizer.encode(sing_subj, add_special_tokens=False)) + len(tokenizer.encode(['##' + affix], add_special_tokens=False))\n",
    "            ]\n",
    "            all_sentence_templates = [sing_sentence_template, plur_sentence_template, comp_sentence_template]\n",
    "            all_tokenization_types = ['default', 'default', 'artificial']\n",
    "            if article_type == 'definite': \n",
    "                sing_art = sing_art_def\n",
    "                plur_art = plur_art_def\n",
    "            elif article_type == 'indefinite': \n",
    "                sing_art = sing_art_indef\n",
    "                plur_art = plur_art_indef\n",
    "            d = {\n",
    "                'lemma': np.repeat(sing_subj, len(all_sentence_templates)),\n",
    "                'word_form': all_wordforms,\n",
    "                'word_number': all_word_number,\n",
    "                'n_tokens': all_n_tokens,\n",
    "                'tokenization_type': all_tokenization_types,\n",
    "                'sentence': all_sentence_templates,\n",
    "                'target_ART_sing': sing_art,\n",
    "                'target_ART_plur': plur_art,\n",
    "                'article_type': article_type,\n",
    "                'affix': np.repeat(affix, len(all_sentence_templates))\n",
    "            }\n",
    "            gather_df.append(pd.DataFrame(d))\n",
    "    sentence_df = pd.concat(gather_df, ignore_index=True)\n",
    "    return sentence_df\n",
    "\n",
    "### Model Predictions \n",
    "\n",
    "def find_sublist_index(lst, sublist):\n",
    "    \"\"\"Find the first occurrence of sublist in lst.\"\"\"\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i:i+len(sublist)] == sublist:\n",
    "            return i, i + len(sublist)\n",
    "    return None\n",
    "\n",
    "def get_article_predictions(df, data_source): \n",
    "    \"\"\"Predict the likelihood of target definite/indefinite and singular/plural articles.\"\"\"\n",
    "    gather_df = []\n",
    "    for (_, row) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        target_article_sing = row['target_ART_sing']\n",
    "        target_article_plur = row['target_ART_plur']\n",
    "\n",
    "        token_article_sing = tokenizer.encode(target_article_sing, add_special_tokens=False)\n",
    "        token_article_plur = tokenizer.encode(target_article_plur, add_special_tokens=False)\n",
    "\n",
    "        token_mask = tokenizer.encode('[MASK]', add_special_tokens=False)\n",
    "\n",
    "        if row['tokenization_type'] == 'artificial': \n",
    "            token_affix = tokenizer.convert_tokens_to_ids([\"##\" + row['affix']])\n",
    "\n",
    "            lemma = row['lemma']\n",
    "            token_lemma = tokenizer.encode(lemma, add_special_tokens=False)\n",
    "\n",
    "            token_start = tokenizer.encode('[CLS]', add_special_tokens=False)\n",
    "            token_end = tokenizer.encode('[SEP]', add_special_tokens=False)\n",
    "\n",
    "            bulky_token_list = [token_start, token_mask, token_lemma, token_affix, token_end]\n",
    "            flat_token_list = [item for sublist in bulky_token_list for item in sublist]\n",
    "            token_idx = torch.tensor([flat_token_list])\n",
    "            inputs = {'input_ids': token_idx, 'token_type_ids': torch.zeros_like(token_idx), 'attention_mask': torch.ones_like(token_idx)}\n",
    "        elif row['tokenization_type'] == 'default': \n",
    "            inputs = tokenizer(row['sentence'], return_tensors='pt', add_special_tokens=True)\n",
    "        model_token_inputs = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0])\n",
    "        model_token_inputs = ' , '.join(model_token_inputs)\n",
    "        outputs = model(**inputs)\n",
    "        midx = find_sublist_index(inputs[\"input_ids\"][0].tolist(), token_mask)\n",
    "        masked_token_logits = outputs.logits[0][midx[0]]\n",
    "        masked_token_probs = torch.softmax(masked_token_logits, dim=0)\n",
    "        prob_article_sing = masked_token_probs[token_article_sing].item()\n",
    "        prob_article_plur = masked_token_probs[token_article_plur].item()\n",
    "        prob_list = [prob_article_sing, prob_article_plur]\n",
    "        article_list = [target_article_sing, target_article_plur]\n",
    "        \n",
    "        d = {\n",
    "            'lemma': np.repeat(row['lemma'], len(prob_list)),\n",
    "            'word_form': np.repeat(row['word_form'], len(prob_list)),\n",
    "            'word_number': np.repeat(row['word_number'], len(prob_list)),\n",
    "            'n_tokens': np.repeat(row['n_tokens'], len(prob_list)),\n",
    "            'tokenization_type': np.repeat(row['tokenization_type'], len(prob_list)),\n",
    "            'article_probs': prob_list,\n",
    "            'article_number': ['singular', 'plural'],\n",
    "            'article_type': np.repeat(row['article_type'], len(prob_list)),\n",
    "            'article': article_list,\n",
    "            'affix': row['affix'],\n",
    "            'sentence': row['sentence'],\n",
    "            'model_token_inputs': model_token_inputs,\n",
    "            'source': data_source\n",
    "        }\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "    probs_df = pd.concat(gather_df, ignore_index=True)\n",
    "    return probs_df\n",
    "\n",
    "# Create sentences and get predictions for SINGLE-TOKEN plurals\n",
    "df_singletok = make_sentences_df(dct_noun_singletok, df_noun_s)\n",
    "probs_df_singletok = get_article_predictions(df_singletok, data_source='single-token')\n",
    "\n",
    "# Create sentences and get predictions for MULTI-TOKEN, MORPHEMIC plurals\n",
    "df_multitok_morph = make_sentences_df(dct_noun_multi_morph, df_noun_mm)\n",
    "probs_df_multitok_morph = get_article_predictions(df_multitok_morph, data_source='morphemic')\n",
    "\n",
    "# Create sentences and get predictions for MULTI-TOKEN, NONMORPHEMIC plurals\n",
    "df_multitok_nonmorph = make_sentences_df(dct_noun_multi_nonmorph, df_noun_nm)\n",
    "probs_df_multitok_nonmorph = get_article_predictions(df_multitok_nonmorph, data_source='nonmorphemic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df_singletok['surprisal'] = probs_df_singletok['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_morph['surprisal'] = probs_df_multitok_morph['article_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_nonmorph['surprisal'] = probs_df_multitok_nonmorph['article_probs'].apply(lambda x: -np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, morphemic when you expect them to be\n",
    "\n",
    "non_morphemic_list = []\n",
    "for (_,row) in df_multitok_morph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif affix_tokid in plur_tokids:\n",
    "        check_morph = \"morphemic\"\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "        \n",
    "        non_morphemic_list.append(sing)\n",
    "\n",
    "print(non_morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pause to check that the default tokenizations for plurals are, in fact, nonmorphemic when you expect them to be\n",
    "\n",
    "morphemic_list = []\n",
    "for (_,row) in df_multitok_nonmorph.iterrows(): \n",
    "    \n",
    "    sing = row['lemma']\n",
    "    plur = row['word_form']\n",
    "    affix = row['affix']\n",
    "    mod_affix = \"##\" + affix\n",
    "    \n",
    "    sing_tokids = tokenizer.encode(sing,add_special_tokens=False)\n",
    "    \n",
    "    if row['tokenization_type'] == 'artificial': \n",
    "        plur_tokids = tokenizer.encode([sing]+[mod_affix],add_special_tokens=False)\n",
    "        \n",
    "    elif row['tokenization_type'] == 'default': \n",
    "        plur_tokids = tokenizer.encode(plur,add_special_tokens=False)\n",
    "\n",
    "\n",
    "    affix_tokid = tokenizer.encode([mod_affix],add_special_tokens=False)[0]\n",
    "\n",
    "    n_plural_tokens = len(plur_tokids)\n",
    "    \n",
    "    sing_tokens = tokenizer.convert_ids_to_tokens(sing_tokids)\n",
    "    plural_tokens = tokenizer.convert_ids_to_tokens(plur_tokids)\n",
    "    \n",
    "    if n_plural_tokens == 1:\n",
    "        check_morph = \"singular single\"\n",
    "        \n",
    "    elif (affix_tokid in plur_tokids) & (row['tokenization_type']=='default'):\n",
    "        check_morph = \"morphemic\"\n",
    "        \n",
    "        morphemic_list.append(sing)\n",
    "        print(sing,plural_tokens,check_morph)\n",
    "\n",
    "\n",
    "    ### Multi-token, non-morphemic\n",
    "    else:\n",
    "        check_morph = \"non_morphemic\"\n",
    "        \n",
    "print(morphemic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save each of these to a dataframe!\n",
    "\n",
    "savepath = 'results_noisy_article-agreement/'\n",
    "\n",
    "if not os.path.exists(savepath): \n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "probs_df_singletok.to_csv(os.path.join(savepath,'results_singletok.csv'))\n",
    "probs_df_multitok_morph.to_csv(os.path.join(savepath,'results_multitok_morph.csv'))\n",
    "probs_df_multitok_nonmorph.to_csv(os.path.join(savepath,'results_multitok_nonmorph.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 (pyenv)",
   "language": "python",
   "name": "pyenv-3.11.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
