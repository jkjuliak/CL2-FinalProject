{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from create_annotated_dicts import dct_noun_singletok as st, dct_noun_multitok_morph as mm, dct_noun_multitok_nonmorph as nmm\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model.eval()\n",
    "random.seed(42)\n",
    "\n",
    "def introduce_typo(word):\n",
    "    \"\"\"Replaces a random letter in the word with a random letter, resembling a typo.\"\"\"\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    original_char = word[idx]\n",
    "    spanish_letters = list('abcdefghijklmnñopqrstuvwxyz')\n",
    "    if original_char in spanish_letters:\n",
    "        letters = spanish_letters.copy()\n",
    "        letters.remove(original_char)\n",
    "        random_letter = random.choice(letters)\n",
    "        mod_word = word[:idx] + random_letter + word[idx + 1:]\n",
    "        return mod_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "dct_noun_singletok = {introduce_typo(noun): article for noun, article in st.items()}\n",
    "dct_noun_multi_morph = {introduce_typo(noun): article for noun, article in mm.items()}\n",
    "dct_noun_multi_nonmorph = {introduce_typo(noun): article for noun, article in nmm.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "from create_annotated_dicts import dct_noun_singletok as st, dct_noun_multitok_morph as mm, dct_noun_multitok_nonmorph as nmm\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model.eval()\n",
    "\n",
    "def introduce_typo(word):\n",
    "    \"\"\"Replaces a random letter in the word with a random letter, resembling a typo.\"\"\"\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "    original_char = word[idx]\n",
    "    spanish_letters = list('abcdefghijklmnñopqrstuvwxyz')\n",
    "    if original_char.lower() in spanish_letters:\n",
    "        letters = spanish_letters.copy()\n",
    "        letters.remove(original_char.lower())\n",
    "        random_letter = random.choice(letters)\n",
    "        # Preserve the case of the original character\n",
    "        if original_char.isupper():\n",
    "            random_letter = random_letter.upper()\n",
    "        mod_word = word[:idx] + random_letter + word[idx + 1:]\n",
    "        return mod_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Keep original dictionaries without typos\n",
    "dct_noun_singletok = st\n",
    "dct_noun_multi_morph = mm\n",
    "dct_noun_multi_nonmorph = nmm\n",
    "\n",
    "# Introduce typos in words for dataframes\n",
    "def create_dataframe_with_typos(dct_noun):\n",
    "    df_noun = pd.DataFrame({\n",
    "        'whole_word': list(dct_noun.keys())\n",
    "    })\n",
    "    df_noun['whole_word_typo'] = df_noun['whole_word'].apply(introduce_typo)\n",
    "    df_noun['root'] = df_noun['whole_word'].apply(lambda x: x[:-1])  # Assuming plural ends with 's' or 'es'\n",
    "    df_noun['affix'] = df_noun['whole_word'].apply(lambda x: x[-1] if x.endswith('s') else x[-2:])\n",
    "    return df_noun\n",
    "\n",
    "# Create dataframes for each category\n",
    "df_noun_s = create_dataframe_with_typos(dct_noun_singletok)\n",
    "df_noun_mm = create_dataframe_with_typos(dct_noun_multi_morph)\n",
    "df_noun_nm = create_dataframe_with_typos(dct_noun_multi_nonmorph)\n",
    "\n",
    "### Define useful functions\n",
    "\n",
    "def make_sentences_df(dct_noun, df_noun):\n",
    "    \"\"\"Creates sentence templates masking what should be a singular or plural verb.\"\"\"\n",
    "    gather_df = []\n",
    "    for idx, row in tqdm(df_noun.iterrows(), total=df_noun.shape[0]): \n",
    "        noun = row['whole_word']\n",
    "        typoed_noun = row['whole_word_typo']\n",
    "        affix = row['affix']\n",
    "        sing_subj = row['root']\n",
    "        sing_art = dct_noun[noun]\n",
    "        sing_art = str(sing_art[0].upper() + sing_art[1:])\n",
    "        if sing_art == 'El':\n",
    "            plur_art = 'Los'\n",
    "        elif sing_art == 'La': \n",
    "            plur_art = 'Las'\n",
    "\n",
    "        # Set up target singular and plural verbs\n",
    "        sing_verb = 'es'   # Singular form of 'ser'\n",
    "        plur_verb = 'son'  # Plural form of 'ser'\n",
    "\n",
    "        # Create composite subject for artificial tokenization\n",
    "        comp_subj = sing_subj + '##' + affix\n",
    "\n",
    "        # Create sentence templates with masked verb\n",
    "        sing_sentence_template = sing_art + ' ' + sing_subj + ' [MASK]'\n",
    "        plur_sentence_template = plur_art + ' ' + typoed_noun + ' [MASK]'  # Use typoed word\n",
    "        comp_sentence_template = sing_art + ' ' + comp_subj + ' [MASK]'\n",
    "\n",
    "        all_wordforms = [sing_subj, typoed_noun, comp_subj]\n",
    "        all_word_number = ['sing', 'plur', 'plur']\n",
    "        all_n_tokens = [\n",
    "            len(tokenizer.encode(sing_subj, add_special_tokens=False)),\n",
    "            len(tokenizer.encode(typoed_noun, add_special_tokens=False)),\n",
    "            len(tokenizer.encode(sing_subj, add_special_tokens=False)) + len(tokenizer.encode(['##' + affix], add_special_tokens=False))\n",
    "        ]\n",
    "        all_sentence_templates = [sing_sentence_template, plur_sentence_template, comp_sentence_template]\n",
    "        all_tokenization_types = ['default', 'default', 'artificial']\n",
    "\n",
    "        d = {\n",
    "            'lemma': np.repeat(sing_subj, len(all_sentence_templates)),\n",
    "            'word_form': all_wordforms,\n",
    "            'word_number': all_word_number,\n",
    "            'n_tokens': all_n_tokens,\n",
    "            'tokenization_type': all_tokenization_types,\n",
    "            'sentence': all_sentence_templates,\n",
    "            'target_VERB_sing': sing_verb,\n",
    "            'target_VERB_plur': plur_verb,\n",
    "            'article': [sing_art, plur_art, sing_art],  # Include article for artificial tokenization\n",
    "            'affix': np.repeat(affix, len(all_sentence_templates))\n",
    "        }\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "    sentence_df = pd.concat(gather_df, ignore_index=True)\n",
    "    return sentence_df\n",
    "\n",
    "### Model Predictions \n",
    "\n",
    "def find_sublist_index(lst, sublist):\n",
    "    \"\"\"Find the first occurrence of sublist in lst.\"\"\"\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i:i+len(sublist)] == sublist:\n",
    "            return i, i + len(sublist)\n",
    "    return None\n",
    "\n",
    "def get_verb_predictions(df, data_source): \n",
    "    \"\"\"Predict the likelihood of target singular and plural verbs.\"\"\"\n",
    "    gather_df = []\n",
    "    for (_, row) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        target_verb_sing = row['target_VERB_sing']\n",
    "        target_verb_plur = row['target_VERB_plur']\n",
    "\n",
    "        # Tokens for each verb\n",
    "        token_verb_sing = tokenizer.encode(target_verb_sing, add_special_tokens=False)\n",
    "        token_verb_plur = tokenizer.encode(target_verb_plur, add_special_tokens=False)\n",
    "\n",
    "        # Token for mask\n",
    "        token_mask = tokenizer.encode('[MASK]', add_special_tokens=False)\n",
    "\n",
    "        if row['tokenization_type'] == 'artificial': \n",
    "            # Tokens for article and noun\n",
    "            token_article = tokenizer.encode(row['article'], add_special_tokens=False)\n",
    "            token_affix = tokenizer.convert_tokens_to_ids([\"##\" + row['affix']])\n",
    "\n",
    "            lemma = row['lemma']\n",
    "            token_lemma = tokenizer.encode(lemma, add_special_tokens=False)\n",
    "\n",
    "            token_start = tokenizer.encode('[CLS]', add_special_tokens=False)\n",
    "            token_end = tokenizer.encode('[SEP]', add_special_tokens=False)\n",
    "\n",
    "            # Construct tokens manually\n",
    "            bulky_token_list = [token_start, token_article, token_lemma, token_affix, token_mask, token_end]\n",
    "            flat_token_list = [item for sublist in bulky_token_list for item in sublist]\n",
    "            token_idx = torch.tensor([flat_token_list])\n",
    "            inputs = {'input_ids': token_idx, 'token_type_ids': torch.zeros_like(token_idx), 'attention_mask': torch.ones_like(token_idx)}\n",
    "        elif row['tokenization_type'] == 'default': \n",
    "            inputs = tokenizer(row['sentence'], return_tensors='pt', add_special_tokens=True)\n",
    "        model_token_inputs = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0])\n",
    "        model_token_inputs = ' , '.join(model_token_inputs)\n",
    "        outputs = model(**inputs)\n",
    "        midx = find_sublist_index(inputs[\"input_ids\"][0].tolist(), token_mask)\n",
    "        masked_token_logits = outputs.logits[0][midx[0]]\n",
    "        masked_token_probs = torch.softmax(masked_token_logits, dim=0)\n",
    "        prob_verb_sing = masked_token_probs[token_verb_sing].item()\n",
    "        prob_verb_plur = masked_token_probs[token_verb_plur].item()\n",
    "        prob_list = [prob_verb_sing, prob_verb_plur]\n",
    "        verb_list = [target_verb_sing, target_verb_plur]\n",
    "        \n",
    "        d = {\n",
    "            'lemma': np.repeat(row['lemma'], len(prob_list)),\n",
    "            'word_form': np.repeat(row['word_form'], len(prob_list)),\n",
    "            'word_number': np.repeat(row['word_number'], len(prob_list)),\n",
    "            'n_tokens': np.repeat(row['n_tokens'], len(prob_list)),\n",
    "            'tokenization_type': np.repeat(row['tokenization_type'], len(prob_list)),\n",
    "            'verb_probs': prob_list,\n",
    "            'verb_number': ['singular', 'plural'],\n",
    "            'verb': verb_list,\n",
    "            'affix': row['affix'],\n",
    "            'sentence': row['sentence'],\n",
    "            'model_token_inputs': model_token_inputs,\n",
    "            'source': data_source\n",
    "        }\n",
    "        gather_df.append(pd.DataFrame(d))\n",
    "    probs_df = pd.concat(gather_df, ignore_index=True)\n",
    "    return probs_df\n",
    "\n",
    "# Create sentences and get predictions for SINGLE-TOKEN plurals\n",
    "df_singletok = make_sentences_df(dct_noun_singletok, df_noun_s)\n",
    "probs_df_singletok = get_verb_predictions(df_singletok, data_source='single-token')\n",
    "\n",
    "# Create sentences and get predictions for MULTI-TOKEN, MORPHEMIC plurals\n",
    "df_multitok_morph = make_sentences_df(dct_noun_multi_morph, df_noun_mm)\n",
    "probs_df_multitok_morph = get_verb_predictions(df_multitok_morph, data_source='morphemic')\n",
    "\n",
    "# Create sentences and get predictions for MULTI-TOKEN, NONMORPHEMIC plurals\n",
    "df_multitok_nonmorph = make_sentences_df(dct_noun_multi_nonmorph, df_noun_nm)\n",
    "probs_df_multitok_nonmorph = get_verb_predictions(df_multitok_nonmorph, data_source='nonmorphemic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df_singletok['surprisal'] = probs_df_singletok['verb_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_morph['surprisal'] = probs_df_multitok_morph['verb_probs'].apply(lambda x: -np.log(x))\n",
    "probs_df_multitok_nonmorph['surprisal'] = probs_df_multitok_nonmorph['verb_probs'].apply(lambda x: -np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save each of these to a dataframe!\n",
    "\n",
    "savepath = 'results_noisy_verb-agreement/'\n",
    "\n",
    "if not os.path.exists(savepath): \n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "probs_df_singletok.to_csv(os.path.join(savepath,'results_singletok.csv'))\n",
    "probs_df_multitok_morph.to_csv(os.path.join(savepath,'results_multitok_morph.csv'))\n",
    "probs_df_multitok_nonmorph.to_csv(os.path.join(savepath,'results_multitok_nonmorph.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 (pyenv)",
   "language": "python",
   "name": "pyenv-3.11.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
